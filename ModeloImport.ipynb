{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1O7U-_e-9Nqdno8DUqIuWimdri37NLCGb",
      "authorship_tag": "ABX9TyP3wgdlq9uPa0b1iAmWxHoT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 1: Instalación de Librerías\n",
        "# ---\n",
        "# Instala las librerías que no vienen por defecto en Colab y son\n",
        "# necesarias para el pipeline guardado.\n",
        "# Solo necesitas ejecutar esto una vez por sesión (o si el entorno se reinicia).\n",
        "\n",
        "!pip install category_encoders==2.6.3 geopy -q\n",
        "# Nota: Se especifica version de category_encoders por compatibilidad, ajustar si da problemas.\n",
        "#       (La versión usada en el notebook original puede variar, si usaste una diferente\n",
        "#        en el entrenamiento, instálala aquí). XGBoost y Scikit-learn suelen estar\n",
        "#        preinstalados en Colab o ser compatibles."
      ],
      "metadata": {
        "id": "_75YM6zRR4kW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 2: Importaciones y Montar Google Drive\n",
        "# ---\n",
        "# Importa todas las librerías necesarias y monta tu Google Drive\n",
        "# para acceder al archivo del pipeline guardado.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "from google.colab import drive\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# Importar clases específicas es necesario para que joblib pueda deserializar los objetos\n",
        "import category_encoders as ce\n",
        "from sklearn.preprocessing import StandardScaler, SplineTransformer, PolynomialFeatures\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb # Aunque no lo usemos directamente, es necesario para cargar el modelo\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive montado.\")"
      ],
      "metadata": {
        "id": "KLQZcCU3R57y",
        "outputId": "19c3a683-ab92-42e1-804a-d987f19d3e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive montado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 3: Cargar el Pipeline Guardado\n",
        "# ---\n",
        "# Carga el diccionario completo que contiene todos los componentes\n",
        "# del pipeline desde el archivo .joblib.\n",
        "\n",
        "# Asegúrate de que esta ruta coincida EXACTAMENTE con donde guardaste el archivo\n",
        "pipeline_file_path = '/content/drive/MyDrive/ModelosIdealista/pipeline_idealista_completo.joblib' # cite: 1\n",
        "\n",
        "if os.path.exists(pipeline_file_path):\n",
        "    pipeline_components = joblib.load(pipeline_file_path) # cite: 1\n",
        "    print(f\"Pipeline cargado exitosamente desde: {pipeline_file_path}\")\n",
        "    print(f\"Componentes cargados: {list(pipeline_components.keys())}\") # cite: 1\n",
        "else:\n",
        "    print(f\"ERROR: No se encontró el archivo del pipeline en {pipeline_file_path}\")\n",
        "    pipeline_components = None # Poner None si no se carga"
      ],
      "metadata": {
        "id": "2VnP65WrR991",
        "outputId": "44eb75f7-9b47-469a-c64a-1801a990aaac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline cargado exitosamente desde: /content/drive/MyDrive/ModelosIdealista/pipeline_idealista_completo.joblib\n",
            "Componentes cargados: ['scaler_geo', 'kmeans_geo', 'pca_geo', 'target_encoder', 'spline_transformer', 'poly_features', 'scaler_final', 'xgb_model', 'pois', 'floor_map', 'imputation_values', 'cluster_avg_logprice_map', 'spline_input_cols', 'poly_base_cols', 'te_cols', 'scaled_num_cols', 'feature_order']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 4: Definir Datos de Entrada para una Nueva Vivienda\n",
        "# ---\n",
        "# Crea un diccionario con los datos de la vivienda para la que quieres\n",
        "# predecir el precio. Usa los nombres de columna originales ANTES de\n",
        "# cualquier transformación (como se cargaron en el Bloque 2 del notebook original).\n",
        "# Sustituye los valores de ejemplo por los de tu vivienda.\n",
        "\n",
        "datos_nueva_vivienda = {\n",
        "    # --- Características Principales ---\n",
        "    'size': 85,              # Metros cuadrados\n",
        "    'rooms': 3,              # Número de habitaciones\n",
        "    'bathrooms': 2,          # Número de baños\n",
        "    'floor': '2',            # Planta (ej. 'bj', '1', '2', 'atico', etc.) - el pipeline lo manejará\n",
        "    'hasLift': 1,            # 1 si tiene ascensor, 0 si no\n",
        "    'exterior': True,        # True si es exterior, False si es interior, 'Unknown' si no se sabe\n",
        "    'propertyType': 'flat',  # Tipo de propiedad (ej. 'flat', 'penthouse', 'duplex')\n",
        "    'status': 'good',        # Estado (ej. 'good', 'newdevelopment', 'renew')\n",
        "    'numPhotos': 15,         # Número de fotos en el anuncio (o un valor razonable)\n",
        "\n",
        "    # --- Ubicación ---\n",
        "    'latitude': 41.3851,     # Latitud (ej. cerca de Plaça Catalunya)\n",
        "    'longitude': 2.1734,     # Longitud (ej. cerca de Plaça Catalunya)\n",
        "\n",
        "    # --- Parking ---\n",
        "    'hasParking': 0,         # 1 si tiene parking (aunque sea opcional), 0 si no\n",
        "    'isParkingIncludedInPrice': 0 # 1 si el parking (si existe) está incluido, 0 si no\n",
        "\n",
        "    # --- Columnas 'isna_' ---\n",
        "    # Estas se crearán automáticamente en la función de predicción si algún\n",
        "    # valor original de las columnas clave es NaN. Para una entrada manual,\n",
        "    # normalmente no las incluirías aquí a menos que simules un NaN original.\n",
        "    # 'isna_size': 0,\n",
        "    # 'isna_rooms': 0,\n",
        "    # ... etc.\n",
        "}\n",
        "\n",
        "# Convertir el diccionario a un DataFrame de Pandas (con una sola fila)\n",
        "input_df = pd.DataFrame([datos_nueva_vivienda])\n",
        "\n",
        "print(\"DataFrame de entrada creado:\")\n",
        "print(input_df.to_markdown(index=False)) # Usar to_markdown para mejor visualización en Colab"
      ],
      "metadata": {
        "id": "QVNP7XqcSFnN",
        "outputId": "6586a050-34dd-433b-9c1b-b72f6b8b3380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame de entrada creado:\n",
            "|   size |   rooms |   bathrooms |   floor |   hasLift | exterior   | propertyType   | status   |   numPhotos |   latitude |   longitude |   hasParking |   isParkingIncludedInPrice |\n",
            "|-------:|--------:|------------:|--------:|----------:|:-----------|:---------------|:---------|------------:|-----------:|------------:|-------------:|---------------------------:|\n",
            "|     85 |       3 |           2 |       2 |         1 | True       | flat           | good     |          15 |    41.3851 |      2.1734 |            0 |                          0 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 5: Definir la Función de Predicción (LIMPIA Y FINAL)\n",
        "# ---\n",
        "\n",
        "def predict_price(input_data_df, pipeline_components):\n",
        "    \"\"\"\n",
        "    Realiza la predicción de precio para nuevos datos usando el pipeline cargado.\n",
        "    (Resto de la docstring igual)\n",
        "    \"\"\"\n",
        "    # Importaciones locales\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from geopy.distance import geodesic\n",
        "\n",
        "    if pipeline_components is None:\n",
        "        print(\"Error: El pipeline no está cargado.\")\n",
        "        return None\n",
        "\n",
        "    # Crear una copia\n",
        "    df_processed = input_data_df.copy()\n",
        "\n",
        "    # Extraer componentes\n",
        "    floor_map = pipeline_components['floor_map']\n",
        "    imputation_values = pipeline_components['imputation_values']\n",
        "    pois = pipeline_components['pois']\n",
        "    scaler_geo = pipeline_components['scaler_geo']\n",
        "    kmeans_geo = pipeline_components['kmeans_geo']\n",
        "    pca_geo = pipeline_components['pca_geo']\n",
        "    cluster_avg_logprice_map = pipeline_components['cluster_avg_logprice_map']\n",
        "    te = pipeline_components['target_encoder']\n",
        "    te_cols = pipeline_components['te_cols']\n",
        "    spline = pipeline_components['spline_transformer']\n",
        "    spline_input_cols = pipeline_components['spline_input_cols']\n",
        "    poly = pipeline_components['poly_features']\n",
        "    poly_base_cols = pipeline_components['poly_base_cols']\n",
        "    scaler_final = pipeline_components['scaler_final']\n",
        "    scaled_num_cols = pipeline_components['scaled_num_cols']\n",
        "    feature_order = pipeline_components['feature_order']\n",
        "    model = pipeline_components['xgb_model']\n",
        "\n",
        "    print(\"\\n--- Iniciando Preprocesamiento ---\")\n",
        "\n",
        "    # --- 1. Limpieza, Formateo e Imputación ---\n",
        "    print(\"Aplicando limpieza e imputación...\")\n",
        "    # (Código igual que antes...)\n",
        "    median_floor = imputation_values['median_floor']\n",
        "    df_processed['floor'] = pd.to_numeric(df_processed['floor'].replace(floor_map), errors='coerce').fillna(median_floor)\n",
        "    mode_exterior = imputation_values['mode_exterior']\n",
        "    df_processed['exterior'] = pd.to_numeric(df_processed['exterior'].replace({'Unknown': np.nan, True: 1, False: 0}), errors='coerce').fillna(mode_exterior).astype(int)\n",
        "    mode_hasLift = imputation_values['mode_hasLift']\n",
        "    df_processed['hasLift'] = df_processed['hasLift'].fillna(mode_hasLift).astype(int)\n",
        "    df_processed['status'] = df_processed['status'].fillna('Unknown')\n",
        "    conds = [(df_processed['hasParking']==1)&(df_processed['isParkingIncludedInPrice']==1), (df_processed['hasParking']==1)&(df_processed['isParkingIncludedInPrice']==0), (df_processed['hasParking']==0)]\n",
        "    choices = ['Included','Optional','None']\n",
        "    df_processed['parking_status'] = np.select(conds, choices, default='Unknown')\n",
        "    df_processed.drop(['hasParking','isParkingIncludedInPrice'], axis=1, inplace=True)\n",
        "    numeric_cols_to_impute = ['size','rooms','bathrooms','latitude','longitude']\n",
        "    for c in numeric_cols_to_impute:\n",
        "        df_processed[f'isna_{c}'] = df_processed[c].isna().astype(int)\n",
        "        median_val = imputation_values[f'median_{c}']\n",
        "        df_processed[c] = df_processed[c].fillna(median_val)\n",
        "\n",
        "    # --- 2. Ingeniería de Características Espaciales ---\n",
        "    print(\"Generando características espaciales...\")\n",
        "    # (Código igual que antes...)\n",
        "    coords = list(zip(df_processed['latitude'], df_processed['longitude']))\n",
        "    for name, loc in pois.items():\n",
        "        df_processed[f'DistKm_{name}'] = [geodesic(loc, xy).km for xy in coords]\n",
        "    geo_scaled = scaler_geo.transform(df_processed[['latitude','longitude']])\n",
        "    df_processed['geo_cluster'] = kmeans_geo.predict(geo_scaled)\n",
        "    pca1 = pca_geo.transform(geo_scaled)\n",
        "    df_processed['geo_pca1'] = pca1.flatten()\n",
        "\n",
        "    # Calcular precio medio del cluster y llenar NaNs\n",
        "    df_processed['cluster_avg_logprice'] = df_processed['geo_cluster'].map(cluster_avg_logprice_map)\n",
        "    if isinstance(cluster_avg_logprice_map, (dict, pd.Series)):\n",
        "        map_values = list(cluster_avg_logprice_map.values()) if isinstance(cluster_avg_logprice_map, dict) else cluster_avg_logprice_map.tolist()\n",
        "        global_avg_logprice = np.mean(map_values) if map_values else np.log1p(300000)\n",
        "    else:\n",
        "        global_avg_logprice = np.log1p(300000)\n",
        "    # Usar sintaxis recomendada para fillna para evitar FutureWarning\n",
        "    df_processed['cluster_avg_logprice'] = df_processed['cluster_avg_logprice'].fillna(global_avg_logprice)\n",
        "\n",
        "\n",
        "    # --- 3. Codificación Categórica (Target Encoding) ---\n",
        "    print(\"Aplicando Target Encoding...\")\n",
        "    df_processed[te_cols] = te.transform(df_processed[te_cols]) # type: ignore\n",
        "\n",
        "    # --- 4. Características No Lineales (Splines y Polinomios) ---\n",
        "    print(\"Generando características no lineales...\")\n",
        "    # (Código igual que antes...)\n",
        "    spline_feats = spline.transform(df_processed[spline_input_cols]) # type: ignore\n",
        "    spline_names = spline.get_feature_names_out(spline_input_cols) # type: ignore\n",
        "    df_splines = pd.DataFrame(spline_feats, columns=spline_names, index=df_processed.index)\n",
        "    df_processed = pd.concat([df_processed, df_splines], axis=1)\n",
        "    X_poly = poly.transform(df_processed[poly_base_cols]) # type: ignore\n",
        "    poly_names = poly.get_feature_names_out(poly_base_cols) # type: ignore\n",
        "    df_poly = pd.DataFrame(X_poly, columns=poly_names, index=df_processed.index)\n",
        "    new_ints = [c for c in poly_names if ' ' in c]\n",
        "    df_processed = pd.concat([df_processed, df_poly[new_ints]], axis=1)\n",
        "\n",
        "\n",
        "    # --- 5. Asegurar Orden y Tipos de Columnas + Escalado Final ---\n",
        "    print(\"Ordenando columnas y aplicando escalado final...\")\n",
        "    # (Código igual que antes...)\n",
        "    for col in feature_order:\n",
        "        if col not in df_processed.columns:\n",
        "            df_processed[col] = 0\n",
        "    df_processed = df_processed[feature_order] # type: ignore\n",
        "    cols_to_scale_present = [col for col in scaled_num_cols if col in df_processed.columns] # type: ignore\n",
        "    if cols_to_scale_present:\n",
        "        for col in cols_to_scale_present:\n",
        "             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "        df_processed[cols_to_scale_present] = df_processed[cols_to_scale_present].fillna(0) # Llenar posibles NaNs de conversión\n",
        "        df_processed[cols_to_scale_present] = scaler_final.transform(df_processed[cols_to_scale_present]) # type: ignore\n",
        "    else:\n",
        "        print(\"Advertencia: No se encontraron columnas para escalar.\")\n",
        "\n",
        "    print(\"--- Preprocesamiento Completado ---\")\n",
        "\n",
        "    # --- 6. Predicción ---\n",
        "    print(\"Realizando predicción...\")\n",
        "    y_pred_log = model.predict(df_processed) # type: ignore\n",
        "    y_pred_orig = np.expm1(y_pred_log)\n",
        "\n",
        "    return y_pred_orig"
      ],
      "metadata": {
        "id": "wSscqrUTSMFH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 6: Realizar la Predicción y Mostrar Resultado\n",
        "# ---\n",
        "# Llama a la función de predicción con los datos de entrada y el pipeline cargado.\n",
        "# Imprime el resultado formateado.\n",
        "\n",
        "if pipeline_components is not None and input_df is not None:\n",
        "    # Realizar la predicción\n",
        "    precio_predicho = predict_price(input_df, pipeline_components)\n",
        "\n",
        "    if precio_predicho is not None:\n",
        "        # Mostrar el resultado (tomando el primer elemento si solo hay una predicción)\n",
        "        precio_final = precio_predicho[0] if len(precio_predicho) == 1 else precio_predicho\n",
        "        print(\"\\n============================================\")\n",
        "        print(f\"  Precio Predicho para la Vivienda:\")\n",
        "        print(f\"  >>> {precio_final:,.2f} € <<<\") # Formateado como moneda\n",
        "        print(\"============================================\")\n",
        "\n",
        "        # Opcional: Mostrar el DataFrame procesado justo antes de la predicción\n",
        "        # print(\"\\nDataFrame final antes de la predicción (primeras 5 columnas):\")\n",
        "        # print(df_processed.iloc[:, :5].head().to_markdown())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se puede realizar la predicción porque el pipeline o los datos de entrada no se cargaron correctamente.\")"
      ],
      "metadata": {
        "id": "YrpGw0V3SM5q",
        "outputId": "86d70d3c-e9e9-4f6d-e012-99f16686b461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iniciando Preprocesamiento ---\n",
            "Aplicando limpieza e imputación...\n",
            "Generando características espaciales...\n",
            "Aplicando Target Encoding...\n",
            "Generando características no lineales...\n",
            "Ordenando columnas y aplicando escalado final...\n",
            "--- Preprocesamiento Completado ---\n",
            "Realizando predicción...\n",
            "\n",
            "============================================\n",
            "  Precio Predicho para la Vivienda:\n",
            "  >>> 457,586.69 € <<<\n",
            "============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4d3694c18cc1>:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_processed['exterior'] = pd.to_numeric(df_processed['exterior'].replace({'Unknown': np.nan, True: 1, False: 0}), errors='coerce').fillna(mode_exterior).astype(int)\n"
          ]
        }
      ]
    }
  ]
}