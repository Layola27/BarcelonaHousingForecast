{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1O7U-_e-9Nqdno8DUqIuWimdri37NLCGb",
      "authorship_tag": "ABX9TyOjnbqu6wAUYk75ewBAH2T8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 0: Instalación de Librerías\n",
        "# ---\n",
        "# Instala la librería 'category_encoders' si no está presente.\n",
        "# Esta librería es necesaria para la codificación de variables categóricas,\n",
        "# específicamente para el Target Encoding que se usará más adelante.\n",
        "# El comando '!' ejecuta un comando de shell desde el notebook.\n",
        "# Solo es necesario ejecutar esto una vez por sesión de Colab/Jupyter.\n",
        "\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "lHCebikOqFWF",
        "outputId": "5c89b5db-0ddc-4b12-9592-0ffa8d7b189a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.11/dist-packages (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.14.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 1: Importaciones y Configuración Inicial\n",
        "# ---\n",
        "# Importa las librerías necesarias y configura el entorno.\n",
        "\n",
        "# Google Colab: Montar Google Drive para acceder a los archivos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Librerías estándar para manipulación de datos y cálculo numérico\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Para visualización (aunque no se usa activamente en este script final)\n",
        "import os # Para interactuar con el sistema operativo (crear carpetas)\n",
        "\n",
        "# Librerías geoespaciales\n",
        "from geopy.distance import geodesic # Para calcular distancias entre coordenadas\n",
        "\n",
        "# Librerías de Scikit-learn para preprocesamiento, clustering, PCA y modelado\n",
        "from sklearn.cluster import KMeans # Para clustering geográfico\n",
        "from sklearn.decomposition import PCA # Para reducción de dimensionalidad geográfica\n",
        "from sklearn.preprocessing import StandardScaler, SplineTransformer, PolynomialFeatures # Para escalado, splines y características polinómicas\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV # Para dividir datos y buscar hiperparámetros\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Para evaluar el modelo\n",
        "\n",
        "# Librería para Gradient Boosting\n",
        "import xgboost as xgb # Modelo principal que se entrenará\n",
        "\n",
        "# Librería para codificación de categóricas\n",
        "import category_encoders as ce # Específicamente para TargetEncoder\n",
        "\n",
        "# Librería para guardar/cargar objetos Python (modelos, scalers, etc.)\n",
        "import joblib\n",
        "\n",
        "# Configuración de la semilla aleatoria para reproducibilidad\n",
        "# Cualquier operación estocástica (train_test_split, KMeans, PCA, XGBoost, RandomizedSearch)\n",
        "# usará esta semilla para producir los mismos resultados en diferentes ejecuciones.\n",
        "RNG = 42"
      ],
      "metadata": {
        "id": "E0h8ZnUVqFT2",
        "outputId": "cf5175ad-2a2d-4c31-d037-5a27acc3e54b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 2: Carga de Datos y Selección Inicial de Columnas\n",
        "# ---\n",
        "# Carga el dataset limpio desde Google Drive y selecciona las columnas\n",
        "# que se consideran relevantes para el modelo inicial.\n",
        "\n",
        "# Ruta al archivo CSV en Google Drive\n",
        "# Asegúrate de que esta ruta sea correcta en tu Drive.\n",
        "file_path = '/content/drive/MyDrive/Dataset Idealista/pisosBarcelona-21-04-2025-clean.csv' # cite: 1\n",
        "\n",
        "# Carga el archivo CSV en un DataFrame de Pandas\n",
        "# Se especifica 'encoding='latin1'' por si hay caracteres especiales no estándar.\n",
        "df = pd.read_csv(file_path, encoding='latin1') # cite: 1\n",
        "\n",
        "# Lista de columnas consideradas relevantes para el modelo\n",
        "# Se seleccionan características sobre el precio, tamaño, distribución, ubicación,\n",
        "# extras (ascensor, parking) y metadatos (fotos, estado).\n",
        "relevant_cols = [\n",
        "    'price','size','rooms','bathrooms','floor','hasLift','exterior', # cite: 1\n",
        "    'propertyType','status','numPhotos','latitude','longitude', # cite: 1\n",
        "    'hasParking','isParkingIncludedInPrice' # cite: 1\n",
        "]\n",
        "\n",
        "# Crea una copia del DataFrame original conteniendo solo las columnas relevantes\n",
        "# Es buena práctica trabajar sobre una copia para no modificar el DataFrame original.\n",
        "df_model = df[relevant_cols].copy() # cite: 1\n",
        "\n",
        "# Imprime las dimensiones del DataFrame cargado para verificación\n",
        "print(f\"Cargados {df_model.shape[0]} registros y {df_model.shape[1]} columnas\") # cite: 1"
      ],
      "metadata": {
        "id": "zdftER3YqFRg",
        "outputId": "219d205b-3336-4090-d492-d20170e0a47e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargados 8478 registros y 14 columnas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 3: Limpieza, Formateo e Imputación de Nulos\n",
        "# ---\n",
        "# Prepara las columnas seleccionadas: convierte tipos, maneja valores especiales,\n",
        "# combina características e imputa valores faltantes (NaN).\n",
        "\n",
        "# Crear una copia explícita para evitar SettingWithCopyWarning\n",
        "df_model = df_model.copy()\n",
        "\n",
        "# 1. Procesamiento de la columna 'floor'\n",
        "#    - Mapeo: Convierte valores de texto ('bj', 'en', 'ss') a numéricos (0, 0.5, -1). # cite: 1\n",
        "#    - Conversión: Intenta convertir toda la columna a numérica. Los errores (ej. 'ático') se fuerzan a NaN.\n",
        "#    - Imputación: Rellena los NaN resultantes (incluyendo los originales y los forzados)\n",
        "#                 con la mediana de los valores numéricos *después* del mapeo inicial.\n",
        "floor_map = {'bj': 0.0, 'en': 0.5, 'ss': -1.0} # cite: 1\n",
        "# Primero, calcula la mediana de los valores mapeados (ignorando NaNs)\n",
        "median_floor_mapped = df_model['floor'].replace(floor_map).apply(pd.to_numeric, errors='coerce').median()\n",
        "# Luego, aplica el mapeo, convierte a numérico y rellena NaN con la mediana calculada\n",
        "df_model['floor'] = pd.to_numeric(\n",
        "    df_model['floor'].replace(floor_map),\n",
        "    errors='coerce'\n",
        ").fillna(median_floor_mapped) # cite: 1\n",
        "\n",
        "# 2. Procesamiento de la columna 'exterior'\n",
        "#    - Mapeo: Convierte True a 1, False a 0, y 'Unknown' a NaN. # cite: 1\n",
        "#    - Conversión: Intenta convertir a numérico (forzando errores a NaN, aunque no debería haberlos tras el mapeo).\n",
        "#    - Imputación: Rellena los NaN con la moda (valor más frecuente: 0 o 1) de la columna. # cite: 1\n",
        "#    - Tipo final: Convierte la columna a tipo entero.\n",
        "mode_exterior = pd.to_numeric(df_model['exterior'].replace({'Unknown':np.nan, True:1, False:0}), errors='coerce').mode()[0]\n",
        "df_model['exterior'] = pd.to_numeric(\n",
        "    df_model['exterior'].replace({'Unknown':np.nan, True:1, False:0}),\n",
        "    errors='coerce'\n",
        ").fillna(mode_exterior).astype(int) # cite: 1\n",
        "\n",
        "# 3. Procesamiento de la columna 'hasLift'\n",
        "#    - Imputación: Rellena los NaN directamente con la moda (0 o 1). # cite: 1\n",
        "#    - Tipo final: Convierte la columna a tipo entero.\n",
        "mode_hasLift = df_model['hasLift'].mode()[0]\n",
        "df_model['hasLift'] = df_model['hasLift'].fillna(mode_hasLift).astype(int) # cite: 1\n",
        "\n",
        "# 4. Procesamiento de la columna 'status'\n",
        "#    - Imputación: Rellena los NaN con la cadena 'Unknown'. # cite: 1\n",
        "df_model['status'] = df_model['status'].fillna('Unknown')\n",
        "\n",
        "# 5. Combinación de características de Parking\n",
        "#    - Crea una nueva columna 'parking_status' con tres categorías:\n",
        "#      'Included': Tiene parking y está incluido en el precio.\n",
        "#      'Optional': Tiene parking pero NO está incluido (se paga aparte).\n",
        "#      'None': No tiene parking.\n",
        "#    - Usa np.select para aplicar esta lógica condicional.\n",
        "#    - Elimina las columnas originales 'hasParking' e 'isParkingIncludedInPrice'.\n",
        "conds = [\n",
        "    (df_model['hasParking']==1)&(df_model['isParkingIncludedInPrice']==1), # cite: 1\n",
        "    (df_model['hasParking']==1)&(df_model['isParkingIncludedInPrice']==0), # cite: 1\n",
        "    (df_model['hasParking']==0) # cite: 1\n",
        "]\n",
        "choices = ['Included','Optional','None'] # cite: 1\n",
        "df_model['parking_status'] = np.select(conds, choices, default='Unknown') # cite: 1\n",
        "df_model.drop(['hasParking','isParkingIncludedInPrice'], axis=1, inplace=True) # cite: 1\n",
        "\n",
        "# 6. Creación de Flags para Nulos en Numéricos Clave\n",
        "#    - Antes de imputar Nulos en columnas numéricas importantes, crea columnas \"flag\" (indicadoras).\n",
        "#    - Estas flags (0 o 1) indican si el valor original en esa fila era nulo o no.\n",
        "#    - Esto permite al modelo saber qué valores fueron imputados, lo cual puede ser informativo.\n",
        "numeric_cols_to_impute = ['size','rooms','bathrooms','latitude','longitude']\n",
        "for c in numeric_cols_to_impute:\n",
        "    df_model[f'isna_{c}'] = df_model[c].isna().astype(int) # cite: 1\n",
        "\n",
        "# 7. Imputación de Nulos en Numéricos Clave\n",
        "#    - Rellena los valores NaN restantes en estas columnas con sus respectivas medianas. # cite: 1\n",
        "#    - Guardamos las medianas calculadas para usarlas en la predicción de nuevos datos.\n",
        "imputation_values = {} # Diccionario para guardar valores de imputación\n",
        "for c in numeric_cols_to_impute:\n",
        "    median_val = df_model[c].median() # cite: 1\n",
        "    df_model[c] = df_model[c].fillna(median_val)\n",
        "    imputation_values[f'median_{c}'] = median_val # Guardar mediana\n",
        "\n",
        "# Guardar también la mediana/moda de otras columnas imputadas antes\n",
        "imputation_values['median_floor'] = median_floor_mapped\n",
        "imputation_values['mode_exterior'] = mode_exterior\n",
        "imputation_values['mode_hasLift'] = mode_hasLift\n",
        "\n",
        "# 8. Verificación Final de Nulos\n",
        "#    - Comprueba si quedan valores nulos en el DataFrame después de toda la limpieza.\n",
        "print(\"\\nValores nulos restantes tras limpieza e imputación:\")\n",
        "print(df_model.isna().sum())"
      ],
      "metadata": {
        "id": "fcRxdDH0qFP2",
        "outputId": "c0cade46-1007-4d32-e087-5df8ea49811c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valores nulos restantes tras limpieza e imputación:\n",
            "price             0\n",
            "size              0\n",
            "rooms             0\n",
            "bathrooms         0\n",
            "floor             0\n",
            "hasLift           0\n",
            "exterior          0\n",
            "propertyType      0\n",
            "status            0\n",
            "numPhotos         0\n",
            "latitude          0\n",
            "longitude         0\n",
            "parking_status    0\n",
            "isna_size         0\n",
            "isna_rooms        0\n",
            "isna_bathrooms    0\n",
            "isna_latitude     0\n",
            "isna_longitude    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-90f4b6613808>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  mode_exterior = pd.to_numeric(df_model['exterior'].replace({'Unknown':np.nan, True:1, False:0}), errors='coerce').mode()[0]\n",
            "<ipython-input-52-90f4b6613808>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_model['exterior'].replace({'Unknown':np.nan, True:1, False:0}),\n",
            "<ipython-input-52-90f4b6613808>:38: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_model['hasLift'] = df_model['hasLift'].fillna(mode_hasLift).astype(int) # cite: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 4: Ingeniería de Características Espaciales\n",
        "# ---\n",
        "# Crea nuevas características basadas en la ubicación geográfica (latitud, longitud).\n",
        "\n",
        "# 1. Cálculo de Distancias a Puntos de Interés (POIs)\n",
        "#    - Define un diccionario con nombres y coordenadas (lat, lon) de POIs relevantes en Barcelona.\n",
        "pois = {\n",
        "  'Catalunya':(41.3874,2.1700), # cite: 1\n",
        "  'Barceloneta':(41.3790,2.1885), # cite: 1\n",
        "  'Sants':(41.3793,2.1400), # cite: 1\n",
        "  'CampNou':(41.3809,2.1228), # cite: 1\n",
        "  'ParcGuell':(41.4145,2.1527) # cite: 1\n",
        "}\n",
        "#    - Obtiene las coordenadas de cada piso en el DataFrame.\n",
        "coords = list(zip(df_model['latitude'],df_model['longitude'])) # cite: 1\n",
        "#    - Para cada POI, calcula la distancia geodésica (en km) desde cada piso al POI.\n",
        "#    - Añade cada conjunto de distancias como una nueva columna al DataFrame.\n",
        "for name, loc in pois.items():\n",
        "    df_model[f'DistKm_{name}'] = [\n",
        "        geodesic(loc, xy).km for xy in coords\n",
        "    ] # cite: 1\n",
        "\n",
        "# 2. Clustering Geográfico y PCA\n",
        "#    - Escala las coordenadas (latitud, longitud) usando StandardScaler.\n",
        "#      Esto es importante para que KMeans y PCA no se vean sesgados por la escala de las variables.\n",
        "#      Guardamos el scaler ajustado ('scaler_geo') para usarlo en predicciones.\n",
        "scaler_geo = StandardScaler() # cite: 1 (modified)\n",
        "geo = scaler_geo.fit_transform(df_model[['latitude','longitude']]) # cite: 1 (modified)\n",
        "\n",
        "#    - Aplica KMeans a las coordenadas escaladas para agrupar los pisos en clusters geográficos.\n",
        "#      'n_clusters=8' define el número de zonas geográficas.\n",
        "#      'random_state=RNG' asegura reproducibilidad.\n",
        "#      'n_init='auto'' es la configuración recomendada para evitar warnings.\n",
        "#      Guardamos el modelo KMeans ajustado ('kmeans') para usarlo en predicciones.\n",
        "kmeans = KMeans(n_clusters=8, random_state=RNG, n_init='auto').fit(geo) # cite: 1\n",
        "#      Añade la etiqueta del cluster asignado a cada piso como una nueva columna.\n",
        "df_model['geo_cluster'] = kmeans.labels_ # cite: 1\n",
        "\n",
        "#    - Aplica PCA (Análisis de Componentes Principales) a las coordenadas escaladas.\n",
        "#      Reduce las dos dimensiones (lat, lon) a una sola componente principal ('n_components=1').\n",
        "#      Esta componente captura la mayor varianza direccional de los datos geográficos.\n",
        "#      Guardamos el objeto PCA ajustado ('pca_geo') para usarlo en predicciones.\n",
        "pca_geo = PCA(n_components=1, random_state=RNG) # cite: 1 (modified)\n",
        "pca1 = pca_geo.fit_transform(geo) # cite: 1 (modified)\n",
        "#      Añade esta componente principal como una nueva columna.\n",
        "df_model['geo_pca1'] = pca1.flatten() # cite: 1\n",
        "\n",
        "# 3. Precio Medio Logarítmico por Cluster Geográfico\n",
        "#    - Calcula el logaritmo natural del precio + 1 (log1p) para reducir el sesgo de precios altos\n",
        "#      y estabilizar la varianza. Esta será la variable objetivo ('target') para el modelo.\n",
        "df_model['price_log'] = np.log1p(df_model['price']) # cite: 1\n",
        "#    - Calcula el precio medio logarítmico para cada cluster geográfico ('geo_cluster').\n",
        "#      'transform('mean')' asigna la media del cluster a cada piso perteneciente a ese cluster.\n",
        "cluster_avg = df_model.groupby('geo_cluster')['price_log'].transform('mean') # cite: 1\n",
        "#    - Añade este precio medio logarítmico del cluster como una nueva característica.\n",
        "#      Esto captura la tendencia general de precios de la zona geográfica del piso.\n",
        "df_model['cluster_avg_logprice'] = cluster_avg # cite: 1\n",
        "#    - Guardamos el mapeo cluster -> avg_logprice para usarlo en predicciones.\n",
        "cluster_avg_logprice_map = df_model.groupby('geo_cluster')['price_log'].mean() # cite: 1\n",
        "\n",
        "print(\"\\nNuevas columnas geoespaciales creadas:\")\n",
        "print(df_model[['latitude', 'longitude', 'DistKm_Catalunya', 'geo_cluster', 'geo_pca1', 'cluster_avg_logprice']].head())"
      ],
      "metadata": {
        "id": "suOZaNsEqFNq",
        "outputId": "5adefe99-9f9f-4b5c-8804-ad3cdd410fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nuevas columnas geoespaciales creadas:\n",
            "    latitude  longitude  DistKm_Catalunya  geo_cluster  geo_pca1  \\\n",
            "0  41.383123   2.130053          3.374851            3 -0.694130   \n",
            "1  41.428589   2.183113          4.704104            2  0.879598   \n",
            "2  41.399013   2.149954          2.115255            0 -0.125673   \n",
            "3  41.422941   2.119466          5.782394            2 -0.092071   \n",
            "4  41.402096   2.188198          2.231663            2  0.452353   \n",
            "\n",
            "   cluster_avg_logprice  \n",
            "0             12.573056  \n",
            "1             12.715842  \n",
            "2             13.148783  \n",
            "3             12.715842  \n",
            "4             12.715842  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 5: Codificación de Variables Categóricas (Target Encoding)\n",
        "# ---\n",
        "# Convierte variables categóricas seleccionadas a numéricas usando Target Encoding.\n",
        "# Target Encoding reemplaza cada categoría con la media (suavizada) de la variable\n",
        "# objetivo ('price_log') para esa categoría.\n",
        "\n",
        "# Lista de columnas categóricas a codificar con Target Encoding.\n",
        "te_cols = ['propertyType','status','parking_status'] # cite: 1\n",
        "\n",
        "# Inicializa el TargetEncoder.\n",
        "# 'cols=te_cols' especifica las columnas a codificar.\n",
        "# 'smoothing=10' es un parámetro de regularización que ayuda a evitar el overfitting,\n",
        "# especialmente para categorías con pocas muestras. Mezcla la media de la categoría\n",
        "# con la media global de la variable objetivo.\n",
        "te = ce.TargetEncoder(cols=te_cols, smoothing=10) # cite: 1\n",
        "\n",
        "# Ajusta el TargetEncoder al conjunto de datos y transforma las columnas especificadas.\n",
        "# Se usa la variable objetivo 'price_log' para calcular las medias por categoría.\n",
        "# Las columnas originales en 'df_model' son reemplazadas por sus versiones codificadas.\n",
        "# Guardamos el encoder ajustado ('te') para usarlo en predicciones.\n",
        "df_model[te_cols] = te.fit_transform(df_model[te_cols], df_model['price_log']) # cite: 1\n",
        "\n",
        "print(\"\\nColumnas categóricas tras Target Encoding:\")\n",
        "print(df_model[te_cols].head())"
      ],
      "metadata": {
        "id": "DGvLwTnPqFLI",
        "outputId": "fe5de4d9-0b7a-4e76-c51a-459733f867d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Columnas categóricas tras Target Encoding:\n",
            "   propertyType     status  parking_status\n",
            "0     12.632989  12.738528       12.593344\n",
            "1     12.632989  12.738528       12.593344\n",
            "2     12.632989  12.738528       12.593344\n",
            "3     13.466743  12.738528       12.593344\n",
            "4     12.632989  12.738528       12.593344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 6: Creación de Características No Lineales (Splines y Polinomios)\n",
        "# ---\n",
        "# Genera nuevas características para capturar relaciones no lineales entre\n",
        "# las variables existentes y la variable objetivo.\n",
        "\n",
        "# 1. Características Spline\n",
        "#    - Los splines permiten modelar relaciones no lineales suaves.\n",
        "#    - Se aplica SplineTransformer a la columna 'size' y a las distancias a POIs.\n",
        "spline_input_cols = ['size'] + [f'DistKm_{p}' for p in pois] # cite: 1 (derived)\n",
        "#    - 'degree=3' usa splines cúbicos.\n",
        "#    - 'n_knots=5' define el número de puntos (nudos) donde las piezas del spline se unen.\n",
        "#    - 'include_bias=False' evita añadir una columna de sesgo (intercepto).\n",
        "spline = SplineTransformer(degree=3, n_knots=5, include_bias=False) # cite: 1\n",
        "#    - Ajusta el transformador y genera las nuevas características spline.\n",
        "#    - Guardamos el transformador ajustado ('spline') para usarlo en predicciones.\n",
        "spline_feats = spline.fit_transform(df_model[spline_input_cols]) # cite: 1\n",
        "#    - Obtiene los nombres de las nuevas características spline generadas.\n",
        "spline_names = spline.get_feature_names_out(spline_input_cols) # cite: 1\n",
        "#    - Crea un DataFrame con las nuevas características spline.\n",
        "df_splines = pd.DataFrame(spline_feats, columns=spline_names, index=df_model.index) # cite: 1\n",
        "#    - Concatena el DataFrame original con el de las características spline.\n",
        "df_model = pd.concat([df_model, df_splines], axis=1) # cite: 1\n",
        "\n",
        "# 2. Características Polinómicas (Interacciones)\n",
        "#    - Genera términos de interacción entre pares de características numéricas base.\n",
        "#    - Selecciona un conjunto de características numéricas base.\n",
        "num_base = ['size','rooms','bathrooms','floor','numPhotos',\n",
        "            'geo_pca1','cluster_avg_logprice'] # cite: 1\n",
        "#    - Inicializa PolynomialFeatures.\n",
        "#    - 'degree=2' calcula interacciones hasta grado 2 (pares de variables).\n",
        "#    - 'interaction_only=True' genera solo términos de interacción (ej. a*b), no términos cuadrados (ej. a^2). # cite: 1\n",
        "#    - 'include_bias=False' evita añadir una columna de unos (intercepto).\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) # cite: 1\n",
        "#    - Ajusta el transformador y genera las características (originales + interacciones).\n",
        "#    - Guardamos el objeto ajustado ('poly') para usarlo en predicciones.\n",
        "X_poly = poly.fit_transform(df_model[num_base]) # cite: 1\n",
        "#    - Obtiene los nombres de todas las características generadas.\n",
        "poly_names = poly.get_feature_names_out(num_base) # cite: 1\n",
        "#    - Crea un DataFrame con todas las características polinómicas.\n",
        "df_poly = pd.DataFrame(X_poly, columns=poly_names, index=df_model.index) # cite: 1\n",
        "#    - Selecciona solo los nombres de las *nuevas* características de interacción (las que contienen '*'). # cite: 1\n",
        "new_ints = [c for c in poly_names if ' ' in c] # Nombres generados por sklearn usan espacio como separador, no '*'\n",
        "#    - Concatena el DataFrame original solo con las *nuevas* interacciones.\n",
        "df_model = pd.concat([df_model, df_poly[new_ints]], axis=1) # cite: 1\n",
        "\n",
        "print(f\"\\nNúmero total de columnas tras añadir splines e interacciones: {df_model.shape[1]}\")"
      ],
      "metadata": {
        "id": "R7KFar2XqFJL",
        "outputId": "b6a340cb-1e75-4244-ea8f-d18130267054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número total de columnas tras añadir splines e interacciones: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 7: Filtrado de Outliers (Valores Atípicos)\n",
        "# ---\n",
        "# Elimina registros con precios extremadamente altos o bajos para evitar\n",
        "# que influyan desproporcionadamente en el entrenamiento del modelo.\n",
        "\n",
        "# Define la variable objetivo (target) que se usará para filtrar.\n",
        "y = df_model['price_log'] # cite: 1\n",
        "\n",
        "# Calcula los percentiles 1 y 99 de la variable objetivo. # cite: 1\n",
        "low, high = y.quantile([0.01, 0.99]) # cite: 1\n",
        "\n",
        "# Crea una máscara booleana: True para los registros cuyo precio logarítmico\n",
        "# está entre el percentil 1 y el 99 (inclusive).\n",
        "mask = y.between(low, high) # cite: 1\n",
        "\n",
        "# Filtra el DataFrame, manteniendo solo los registros donde la máscara es True.\n",
        "df_model = df_model[mask] # cite: 1\n",
        "\n",
        "# Imprime el número de registros restantes después del filtrado.\n",
        "print(f\"Número de registros tras filtrar outliers (percentiles 1-99 de price_log): {df_model.shape[0]}\") # cite: 1"
      ],
      "metadata": {
        "id": "4Qgx9nyHqFGs",
        "outputId": "ecf74a09-740a-4942-d7ca-5e5e57b3bf2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de registros tras filtrar outliers (percentiles 1-99 de price_log): 8309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 8: División en Conjuntos de Entrenamiento y Prueba y Escalado Final\n",
        "# ---\n",
        "# Prepara los datos para el entrenamiento del modelo: separa características (X)\n",
        "# y objetivo (y), divide en entrenamiento y prueba, y escala las características numéricas.\n",
        "\n",
        "# 1. Separación de Características (X) y Variable Objetivo (y)\n",
        "#    - 'X' contendrá todas las columnas excepto el precio original y el precio logarítmico.\n",
        "#    - 'y' contendrá la variable objetivo ('price_log') que el modelo intentará predecir.\n",
        "X = df_model.drop(columns=['price','price_log']) # cite: 1\n",
        "y = df_model['price_log'] # cite: 1\n",
        "\n",
        "# 2. División en Conjuntos de Entrenamiento y Prueba\n",
        "#    - Divide X e y en conjuntos de entrenamiento (80%) y prueba (20%).\n",
        "#    - 'test_size=0.2' especifica la proporción del conjunto de prueba.\n",
        "#    - 'random_state=RNG' asegura que la división sea la misma cada vez que se ejecute.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RNG\n",
        ") # cite: 1\n",
        "\n",
        "# 3. Escalado de Características Numéricas Finales\n",
        "#    - Selecciona todas las columnas numéricas (int64, float64) del conjunto de entrenamiento.\n",
        "#      Estas incluyen las originales, las creadas (distancias, PCA, target encoded, splines, interacciones), y las flags isna.\n",
        "num_cols = X_train.select_dtypes(include=np.number).columns # cite: 1 (modified np.number covers int/float)\n",
        "#    - Inicializa StandardScaler. Este transformador escala los datos para que tengan media 0 y desviación estándar 1.\n",
        "scaler = StandardScaler() # cite: 1\n",
        "#    - Ajusta ('fit') el scaler *solo* con los datos de entrenamiento ('X_train').\n",
        "#      Esto calcula la media y desviación estándar de cada columna numérica en el conjunto de entrenamiento.\n",
        "scaler.fit(X_train[num_cols]) # cite: 1\n",
        "#    - Transforma ('transform') los datos de entrenamiento y prueba usando el scaler ajustado.\n",
        "#      Es crucial usar el mismo scaler ajustado en ambos conjuntos para evitar fuga de datos (data leakage).\n",
        "#      Guardamos el scaler ajustado ('scaler') para usarlo en predicciones.\n",
        "X_train[num_cols] = scaler.transform(X_train[num_cols]) # cite: 1\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols]) # cite: 1\n",
        "\n",
        "# Imprime las dimensiones de los conjuntos resultantes\n",
        "print(\"Dimensiones de los conjuntos de datos:\")\n",
        "print(f\"X_train: {X_train.shape}\") # cite: 1\n",
        "print(f\"X_test : {X_test.shape}\") # cite: 1\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"y_test : {y_test.shape}\")\n",
        "\n",
        "# Guarda el orden final de las columnas para asegurar consistencia en predicción\n",
        "feature_order = X_train.columns.tolist() # cite: 1 (derived)"
      ],
      "metadata": {
        "id": "Zq3xD7wzqE5-",
        "outputId": "ae59c903-f8e6-4bd4-940d-fef0fe17a9f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de los conjuntos de datos:\n",
            "X_train: (6647, 82)\n",
            "X_test : (1662, 82)\n",
            "y_train: (6647,)\n",
            "y_test : (1662,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 9: Búsqueda de Hiperparámetros y Entrenamiento del Modelo XGBoost\n",
        "# ---\n",
        "# Utiliza RandomizedSearchCV para encontrar la mejor combinación de hiperparámetros\n",
        "# para un modelo XGBoost Regressor y entrena el modelo final con esos parámetros.\n",
        "\n",
        "# 1. Definición del Espacio de Búsqueda de Hiperparámetros\n",
        "#    - Define un diccionario donde las claves son los nombres de los hiperparámetros\n",
        "#      de XGBoost y los valores son listas o distribuciones de posibles valores a probar.\n",
        "param_dist = {\n",
        "    'n_estimators':     [100, 200, 300, 500],       # Número de árboles (boosting rounds) # cite: 1 (values modified for illustration)\n",
        "    'learning_rate':    [0.01, 0.05, 0.1, 0.2],     # Tasa de aprendizaje # cite: 1 (values modified for illustration)\n",
        "    'max_depth':        [3, 5, 7, 9],               # Profundidad máxima de cada árbol # cite: 1 (values modified for illustration)\n",
        "    'subsample':        [0.7, 0.8, 0.9, 1.0],       # Fracción de muestras usadas para entrenar cada árbol # cite: 1\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],       # Fracción de características usadas para entrenar cada árbol # cite: 1\n",
        "    'reg_alpha':        [0, 0.01, 0.1, 0.5, 1],     # Regularización L1 (Lasso) # cite: 1 (values modified for illustration)\n",
        "    'reg_lambda':       [0.1, 0.5, 1, 2, 5]         # Regularización L2 (Ridge) # cite: 1 (values modified for illustration)\n",
        "}\n",
        "\n",
        "# 2. Inicialización del Modelo Base XGBoost\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror', # Función objetivo: error cuadrático (para regresión) # cite: 1\n",
        "    random_state=RNG,             # Semilla aleatoria para reproducibilidad # cite: 1\n",
        "    n_jobs=-1,                    # Usar todos los cores de CPU disponibles # cite: 1\n",
        "    tree_method='hist'            # Algoritmo eficiente para construir árboles, bueno para datasets grandes # cite: 1\n",
        ")\n",
        "\n",
        "# 3. Inicialización de RandomizedSearchCV\n",
        "#    - Realiza una búsqueda aleatoria de hiperparámetros en lugar de probar todas las combinaciones (GridSearch).\n",
        "#    - 'xgb_model': El estimador base a optimizar.\n",
        "#    - 'param_dist': El espacio de búsqueda definido anteriormente.\n",
        "#    - 'n_iter=50': Número de combinaciones de parámetros a probar (ajustar según tiempo/recursos). # cite: 1 (value modified for illustration)\n",
        "#    - 'cv=5': Número de folds para validación cruzada. # cite: 1\n",
        "#    - 'scoring='neg_mean_absolute_error'': Métrica a optimizar (negativo porque scikit-learn maximiza). # cite: 1\n",
        "#    - 'random_state=RNG': Semilla para reproducibilidad de la selección de combinaciones. # cite: 1\n",
        "#    - 'n_jobs=-1': Usar todos los cores para la validación cruzada. # cite: 1\n",
        "#    - 'verbose=2': Nivel de mensajes durante la búsqueda. # cite: 1\n",
        "#    - 'error_score='raise'': Detener si una combinación de parámetros causa un error. # cite: 1\n",
        "rand_search = RandomizedSearchCV(\n",
        "    xgb_model, param_dist,\n",
        "    n_iter=50, cv=5, # Aumentado n_iter para una mejor búsqueda\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    random_state=RNG, n_jobs=-1, verbose=2,\n",
        "    error_score='raise'\n",
        ")\n",
        "\n",
        "# 4. Ejecución de la Búsqueda y Entrenamiento\n",
        "#    - Ajusta RandomizedSearchCV al conjunto de entrenamiento (X_train, y_train).\n",
        "#    - Esto realiza la validación cruzada para cada una de las 'n_iter' combinaciones\n",
        "#      y encuentra la mejor combinación según la métrica 'scoring'.\n",
        "print(\"Iniciando RandomizedSearchCV para XGBoost...\")\n",
        "rand_search.fit(X_train, y_train) # cite: 1\n",
        "print(\"RandomizedSearchCV completado.\")\n",
        "\n",
        "# 5. Obtención del Mejor Modelo y Parámetros\n",
        "#    - El atributo 'best_estimator_' contiene el modelo XGBoost entrenado con\n",
        "#      la mejor combinación de hiperparámetros encontrada.\n",
        "best_model = rand_search.best_estimator_ # cite: 1\n",
        "#    - El atributo 'best_params_' contiene el diccionario con los mejores hiperparámetros.\n",
        "print(\"\\nMejores hiperparámetros encontrados:\")\n",
        "print(rand_search.best_params_) # cite: 1\n",
        "#    - El atributo 'best_score_' contiene el valor de la métrica 'scoring' para los mejores parámetros.\n",
        "print(f\"\\nMejor puntuación (neg_mean_absolute_error) en validación cruzada: {rand_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "FslUaq7WqYih",
        "outputId": "23482920-47df-4e91-9c84-07cba819b2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando RandomizedSearchCV para XGBoost...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "RandomizedSearchCV completado.\n",
            "\n",
            "Mejores hiperparámetros encontrados:\n",
            "{'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alpha': 0, 'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
            "\n",
            "Mejor puntuación (neg_mean_absolute_error) en validación cruzada: -0.1617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 10: Evaluación Final del Modelo\n",
        "# ---\n",
        "# Evalúa el rendimiento del mejor modelo encontrado ('best_model')\n",
        "# sobre el conjunto de prueba ('X_test', 'y_test'), que el modelo no ha visto durante el entrenamiento.\n",
        "\n",
        "# 1. Predicción sobre el Conjunto de Prueba\n",
        "#    - Usa el modelo entrenado ('best_model') para predecir los valores de 'price_log' para X_test.\n",
        "y_pred_log = best_model.predict(X_test) # cite: 1\n",
        "\n",
        "# 2. Reversión de la Transformación Logarítmica\n",
        "#    - Convierte tanto las predicciones ('y_pred_log') como los valores reales ('y_test')\n",
        "#      de vuelta a la escala original de precios usando la inversa de np.log1p, que es np.expm1.\n",
        "y_test_orig = np.expm1(y_test) # cite: 1\n",
        "y_pred_orig = np.expm1(y_pred_log) # cite: 1\n",
        "\n",
        "# 3. Cálculo de Métricas de Evaluación en Escala Original\n",
        "#    - Calcula métricas de regresión comparando los precios originales predichos y reales.\n",
        "#    - MAE (Mean Absolute Error): Error absoluto medio en €. Mide la desviación promedio. # cite: 1\n",
        "#    - RMSE (Root Mean Squared Error): Raíz del error cuadrático medio en €. Penaliza más los errores grandes. # cite: 1\n",
        "#    - R² (R-squared): Coeficiente de determinación. Indica la proporción de la varianza\n",
        "#                      de la variable objetivo que es explicable por el modelo (mejor cerca de 1). # cite: 1\n",
        "mae  = mean_absolute_error(y_test_orig, y_pred_orig)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
        "r2   = r2_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "# 4. Impresión de las Métricas\n",
        "print(\"\\n--- Evaluación Final del Modelo en Conjunto de Prueba (Escala Original) ---\")\n",
        "print(f\"MAE  (Mean Absolute Error) : {mae:,.2f} €\") # cite: 1\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:,.2f} €\") # cite: 1\n",
        "print(f\"R²   (R-squared)           : {r2:.4f}\") # cite: 1"
      ],
      "metadata": {
        "id": "oU-mwKlWqaZW",
        "outputId": "d5153d9e-5a88-4b3c-daec-0ca1e9c40a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluación Final del Modelo en Conjunto de Prueba (Escala Original) ---\n",
            "MAE  (Mean Absolute Error) : 64,917.25 €\n",
            "RMSE (Root Mean Squared Error): 124,165.43 €\n",
            "R²   (R-squared)           : 0.8630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 11: Guardado del Pipeline Completo (CORREGIDO)\n",
        "# ---\n",
        "# Guarda todos los componentes necesarios (transformadores ajustados, modelo,\n",
        "# metadatos, etc.) en un único archivo usando joblib para poder cargar\n",
        "# y usar el pipeline completo para predicciones futuras sobre datos nuevos.\n",
        "\n",
        "# 1. Definir Carpeta de Destino\n",
        "#    - Especifica la ruta en Google Drive donde se guardará el archivo del pipeline.\n",
        "#    - Crea la carpeta si no existe.\n",
        "save_dir = '/content/drive/MyDrive/ModelosIdealista' # cite: 1\n",
        "os.makedirs(save_dir, exist_ok=True) # cite: 1\n",
        "\n",
        "# 2. Crear el Diccionario del Pipeline Completo\n",
        "#    - Reúne todos los objetos ajustados y la información auxiliar necesaria\n",
        "#      en un diccionario para serializarlo.\n",
        "pipeline_completo = {\n",
        "    # --- Objetos transformadores y modelo ajustados ---\n",
        "    'scaler_geo': scaler_geo,               # Scaler para lat/lon (Bloque 4)\n",
        "    'kmeans_geo': kmeans,                   # KMeans para clusters geo (Bloque 4)\n",
        "    'pca_geo': pca_geo,                     # PCA para componente geo (Bloque 4)\n",
        "    'target_encoder': te,                   # TargetEncoder (Bloque 5) # cite: 1\n",
        "    'spline_transformer': spline,           # SplineTransformer (Bloque 6)\n",
        "    'poly_features': poly,                  # PolynomialFeatures (Bloque 6)\n",
        "    'scaler_final': scaler,                 # Scaler final para todas las numéricas (Bloque 8) # cite: 1\n",
        "    'xgb_model': best_model,                # Modelo XGBoost entrenado (Bloque 9) # cite: 1\n",
        "\n",
        "    # --- Información auxiliar y metadatos ---\n",
        "    'pois': pois,                           # Dict de Puntos de Interés (Bloque 4)\n",
        "    'floor_map': floor_map,                 # Mapeo para columna 'floor' (Bloque 3)\n",
        "    'imputation_values': imputation_values, # Valores (medianas/modas) para imputar NaNs (Bloque 3)\n",
        "    'cluster_avg_logprice_map': cluster_avg_logprice_map, # Mapeo cluster -> avg_price (Bloque 4)\n",
        "    'spline_input_cols': spline_input_cols, # Columnas de entrada para SplineTransformer (Bloque 6)\n",
        "    'poly_base_cols': num_base,             # Columnas base para PolynomialFeatures (Bloque 6) # cite: 1\n",
        "    'te_cols': te_cols,                     # Columnas para TargetEncoder (Bloque 5) # cite: 1\n",
        "    'scaled_num_cols': num_cols,            # Columnas escaladas por scaler_final (Bloque 8) # cite: 1\n",
        "    'feature_order': feature_order          # Orden final de columnas para el modelo (Bloque 8) # cite: 1\n",
        "}\n",
        "\n",
        "# 3. Serializar (Guardar) el Pipeline Completo\n",
        "#    - Define el nombre del archivo (usamos '_completo' para diferenciarlo del original).\n",
        "file_save_path = os.path.join(save_dir, 'pipeline_idealista_completo.joblib')\n",
        "#    - Usa joblib.dump para guardar el diccionario 'pipeline_completo' en el archivo especificado.\n",
        "#      Joblib es eficiente para guardar objetos grandes como modelos de scikit-learn y arrays de numpy.\n",
        "joblib.dump(pipeline_completo, file_save_path) # cite: 1 (modified)\n",
        "\n",
        "# 4. Confirmación\n",
        "print(f\"\\n✅ Pipeline COMPLETO exportado correctamente en: {file_save_path}\")\n",
        "print(f\"   Componentes guardados: {list(pipeline_completo.keys())}\")"
      ],
      "metadata": {
        "id": "ZROvIBF4qgax",
        "outputId": "16b5c8e9-6e3d-47bc-9fed-46295a778d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Pipeline COMPLETO exportado correctamente en: /content/drive/MyDrive/ModelosIdealista/pipeline_idealista_completo.joblib\n",
            "   Componentes guardados: ['scaler_geo', 'kmeans_geo', 'pca_geo', 'target_encoder', 'spline_transformer', 'poly_features', 'scaler_final', 'xgb_model', 'pois', 'floor_map', 'imputation_values', 'cluster_avg_logprice_map', 'spline_input_cols', 'poly_base_cols', 'te_cols', 'scaled_num_cols', 'feature_order']\n"
          ]
        }
      ]
    }
  ]
}