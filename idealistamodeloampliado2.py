# -*- coding: utf-8 -*-
"""IdealistaModeloAmpliado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Layola27/BarcelonaHousingForecast/blob/main/IdealistaModeloAmpliado.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
file_path = "/content/drive/MyDrive/Dataset Idealista/pisos_barcelona_sin_duplicados.csv"
df = pd.read_csv(file_path, encoding='latin1')

df.info()

# Convertir columnas al tipo de dato adecuado
import pandas as pd

# Convertir fechas si hay alguna en el dataset
date_columns = ['publicationDate', 'updatedAt']  # Ajustar si hay columnas de fecha en el dataset
for col in date_columns:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

# Convertir n√∫meros a enteros donde aplique
int_columns = ['numPhotos', 'rooms', 'bathrooms', 'floor']
for col in int_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)

# Convertir precios y m√©tricas econ√≥micas a float
float_columns = ['price', 'size', 'priceByArea']
for col in float_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Mostrar tipos de datos despu√©s de la conversi√≥n
print(df.dtypes)

import matplotlib.pyplot as plt

# Seleccionar solo las columnas num√©ricas
numeric_columns = ['price', 'size', 'rooms', 'bathrooms', 'priceByArea']

# Crear histogramas
plt.figure(figsize=(12, 8))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(2, 3, i)
    plt.hist(df[col].dropna(), bins=50, edgecolor='black', alpha=0.7)
    plt.xlabel(col)
    plt.ylabel("Frecuencia")
    plt.title(f"Distribuci√≥n de {col}")
    plt.yscale("log")  # Escala logar√≠tmica para mejor visualizaci√≥n
    plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

import seaborn as sns

plt.figure(figsize=(12, 6))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(y=df[col])
    plt.ylabel(col)
    plt.title(f"Boxplot de {col}")
    plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

import seaborn as sns
import numpy as np

# Calcular la correlaci√≥n
corr = df[numeric_columns].corr()

# Crear el mapa de calor
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, vmin=-1, vmax=1)
plt.title("Matriz de Correlaci√≥n entre Variables Num√©ricas")
plt.show()

sns.pairplot(df[numeric_columns], diag_kind='kde')
plt.show()

pip install contextily

import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx

# Convertir el DataFrame de pisos en un GeoDataFrame
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df['longitude'], df['latitude']),
    crs="EPSG:4326"
)

# Convertir a sistema de coordenadas proyectadas (EPSG:3857) para trabajar con contextily
gdf = gdf.to_crs(epsg=3857)

# Crear la figura y los ejes
fig, ax = plt.subplots(figsize=(12, 8))

# Graficar los puntos con mejoras en la visualizaci√≥n:
# - markersize aumentado para mayor visibilidad
# - alpha ajustado para ver la superposici√≥n
# - edgecolor para resaltar cada punto
gdf.plot(
    ax=ax,
    markersize=50,
    alpha=0.7,
    color='royalblue',
    edgecolor='k',
    linewidth=0.5,
    label="Pisos en Venta"
)

# Ajustar el l√≠mite del mapa seg√∫n la extensi√≥n de los datos, agregando un peque√±o margen (500 unidades)
minx, miny, maxx, maxy = gdf.total_bounds
ax.set_xlim(minx - 500, maxx + 500)
ax.set_ylim(miny - 500, maxy + 500)

# Agregar el fondo de mapa de OpenStreetMap
ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)

# Para una apariencia m√°s limpia, se pueden ocultar los ejes (opcional)
ax.set_axis_off()

# Agregar t√≠tulo y, si se prefiere, la leyenda (si se ocultan los ejes, la leyenda se deber√° agregar manualmente)
plt.title("Distribuci√≥n de Pisos en Barcelona", fontsize=16, pad=20)
plt.legend(loc="upper right")

# Mostrar el mapa
plt.show()

import folium
from folium.plugins import MarkerCluster

# Coordenadas centrales de Barcelona para centrar el mapa
centro_barcelona = [41.3851, 2.1734]

# Crear el mapa interactivo
mapa = folium.Map(location=centro_barcelona, zoom_start=12, tiles='OpenStreetMap')

# Crear un agrupador de marcadores para una mejor visualizaci√≥n
marker_cluster = MarkerCluster().add_to(mapa)

# Recorrer cada registro del DataFrame y agregar un marcador
for idx, row in df.iterrows():
    lat = row['latitude']
    lon = row['longitude']

    # Crear un popup con informaci√≥n del piso.
    # Puedes incluir m√°s datos (como precio, direcci√≥n, etc.) si est√°n disponibles en tu DataFrame.
    popup_text = f"Piso en Venta<br>Lat: {lat:.4f}<br>Lon: {lon:.4f}"

    folium.Marker(
        location=[lat, lon],
        popup=popup_text,
        icon=folium.Icon(color='blue', icon='home', prefix='fa')
    ).add_to(marker_cluster)

# Mostrar el mapa en Jupyter Notebook (o guardarlo en un archivo HTML para verlo en el navegador)
mapa.save("mapa_barcelona.html")
mapa

import plotly.express as px

# Filtrar valores nulos en coordenadas
df_filtered = df[['latitude', 'longitude']].dropna()

fig = px.density_mapbox(
    df_filtered,
    lat='latitude',
    lon='longitude',
    radius=10,  # Ajusta el radio para modificar la dispersi√≥n de la densidad
    center={'lat': 41.3851, 'lon': 2.1734},
    zoom=10,
    mapbox_style='carto-positron',
    title="Mapa de Densidad de Pisos en Barcelona"
)

fig.show()

import pandas as pd
from sklearn.model_selection import train_test_split

# Copia del dataset original
df_clean = df.copy()

# 1Ô∏è‚É£ Seleccionar columnas num√©ricas relevantes
numerical_cols = [
    "size", "rooms", "bathrooms", "latitude", "longitude", "distance", "numPhotos", "priceByArea"
]

# 2Ô∏è‚É£ Seleccionar columnas categ√≥ricas relevantes
categorical_cols = [
    "propertyType", "operation", "floor", "exterior", "province", "municipality",
    "district", "neighborhood", "status", "hasLift", "newDevelopment"
]

# 3Ô∏è‚É£ Imputar valores faltantes en variables categ√≥ricas
df_clean["floor"].fillna("unknown", inplace=True)
df_clean["exterior"].fillna("unknown", inplace=True)
df_clean["hasLift"].fillna("unknown", inplace=True)
df_clean["district"].fillna("unknown", inplace=True)
df_clean["neighborhood"].fillna("unknown", inplace=True)

# 4Ô∏è‚É£ Aplicar One-Hot Encoding a variables categ√≥ricas
df_encoded = pd.get_dummies(df_clean[categorical_cols], drop_first=True)

# 5Ô∏è‚É£ Combinar con variables num√©ricas y la variable objetivo "price"
df_final = pd.concat([df_clean[numerical_cols], df_encoded, df_clean["price"]], axis=1)

# 6Ô∏è‚É£ Separar variables predictoras (X) y variable objetivo (y)
X = df_final.drop(columns=["price"])
y = df_final["price"]

# 7Ô∏è‚É£ Dividir en conjunto de entrenamiento y prueba (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("‚úÖ Datos preprocesados y listos para entrenar los modelos")
print(f"Tama√±o de entrenamiento: {X_train.shape}, Tama√±o de prueba: {X_test.shape}")

"""Regresi√≥n Lineal"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1Ô∏è‚É£ Separar variables predictoras y variable objetivo
X = df_final.drop(columns=["price"])
y = df_final["price"]

# 2Ô∏è‚É£ Dividir 80% interpolaci√≥n y 20% extrapolaci√≥n
X_interpolation, X_extrapolation, y_interpolation, y_extrapolation = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

# 3Ô∏è‚É£ Dentro de la interpolaci√≥n, dividir 80% train y 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X_interpolation, y_interpolation, test_size=0.2, random_state=42
)

# 4Ô∏è‚É£ Escalar las variables num√©ricas (para modelos que lo requieran)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_extrapolation_scaled = scaler.transform(X_extrapolation)

# 5Ô∏è‚É£ Aplicar regularizaci√≥n con Lasso para eliminar variables irrelevantes
lasso = Lasso(alpha=0.1)  # Ajusta alpha para mayor regularizaci√≥n
lasso.fit(X_train_scaled, y_train)

# Obtener coeficientes de Lasso (variables m√°s importantes)
lasso_importance = np.abs(lasso.coef_)

# 6Ô∏è‚É£ Entrenar modelo Random Forest para obtener importancia de caracter√≠sticas
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

rf_importance = rf_model.feature_importances_

# 7Ô∏è‚É£ Combinar resultados de Lasso y Random Forest en un DataFrame
feature_importance_df = pd.DataFrame({
    "Feature": X.columns,
    "Lasso Importance": lasso_importance,
    "Random Forest Importance": rf_importance
})

# Ordenar por la importancia promedio de ambos m√©todos
feature_importance_df["Mean Importance"] = (
    feature_importance_df["Lasso Importance"] + feature_importance_df["Random Forest Importance"]
) / 2

feature_importance_df = feature_importance_df.sort_values(by="Mean Importance", ascending=False)

# 8Ô∏è‚É£ Agrupar variables categ√≥ricas transformadas con One-Hot Encoding
categorical_original = ["propertyType", "operation", "floor", "exterior", "province", "municipality",
                         "district", "neighborhood", "status", "hasLift", "newDevelopment"]

feature_importance_grouped = {}

for feature, importance in zip(feature_importance_df["Feature"], feature_importance_df["Mean Importance"]):
    base_feature = feature.split("_")[0]  # Obtener la parte base antes del One-Hot Encoding
    if base_feature in categorical_original:
        if base_feature in feature_importance_grouped:
            feature_importance_grouped[base_feature] += importance
        else:
            feature_importance_grouped[base_feature] = importance
    else:
        feature_importance_grouped[feature] = importance  # Mantener variables num√©ricas sin cambios

# Convertir a DataFrame ordenado
feature_importance_grouped_df = pd.DataFrame(
    list(feature_importance_grouped.items()), columns=["Feature", "Importance"]
).sort_values(by="Importance", ascending=False)

# 9Ô∏è‚É£ Mostrar la importancia de cada variable agrupada
print("Importancia de las Variables (Agrupadas por Categor√≠a):")
print(feature_importance_grouped_df)

# üìä  üîü Graficar la importancia de las variables agrupadas
plt.figure(figsize=(12,6))
plt.barh(feature_importance_grouped_df["Feature"], feature_importance_grouped_df["Importance"], color='royalblue')
plt.xlabel("Importancia")
plt.ylabel("Variable")
plt.title("Importancia de las Variables en la Predicci√≥n del Precio (Regularizaci√≥n + Random Forest)")
plt.gca().invert_yaxis()  # Invertir el eje para mostrar la variable m√°s importante arriba
plt.show()

# üìâ Graficar comparaci√≥n de precios reales vs predichos
plt.figure(figsize=(12,6))
plt.plot(df_final.index[:len(y_interpolation)], y_interpolation, label="Precio Real (Interpolaci√≥n)", alpha=0.7)
plt.plot(df_final.index[:len(y_interpolation)], model.predict(scaler.transform(X_interpolation)), label="Precio Predicho (Interpolaci√≥n)", alpha=0.7)
plt.plot(df_final.index[len(y_interpolation):], y_extrapolation, label="Precio Real (Extrapolaci√≥n)", alpha=0.7, linestyle='dashed')
plt.plot(df_final.index[len(y_interpolation):], y_extrapolation_pred, label="Precio Predicho (Extrapolaci√≥n)", alpha=0.7, linestyle='dashed')
plt.xlabel("√çndice (simulando el tiempo)")
plt.ylabel("Precio")
plt.legend()
plt.title("Comparaci√≥n de Precio Real vs Predicho (Interpolaci√≥n y Extrapolaci√≥n)")
plt.show()

# üìà Predicciones
y_test_pred = rf_model.predict(X_test_scaled)
y_extrapolation_pred = rf_model.predict(X_extrapolation_scaled)

# üìä M√©tricas para conjunto de test (Interpolaci√≥n)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)

# üìä M√©tricas para conjunto de extrapolaci√≥n
mse_extra = mean_squared_error(y_extrapolation, y_extrapolation_pred)
rmse_extra = np.sqrt(mse_extra)
r2_extra = r2_score(y_extrapolation, y_extrapolation_pred)

# üì¢ Imprimir resultados
print("üìä M√©tricas de Evaluaci√≥n:")
print(f"üîπ Test (Interpolaci√≥n): MSE = {mse_test:.2f}, RMSE = {rmse_test:.2f}, R¬≤ = {r2_test:.4f}")
print(f"üîπ Extrapolaci√≥n:        MSE = {mse_extra:.2f}, RMSE = {rmse_extra:.2f}, R¬≤ = {r2_extra:.4f}")