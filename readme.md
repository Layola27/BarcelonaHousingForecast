# üèòÔ∏è Proyecto de Predicci√≥n de Precios de Viviendas en Barcelona

## üìå Descripci√≥n del Proyecto

Este proyecto se centra en la construcci√≥n de un modelo de machine learning para predecir los precios de las viviendas en Barcelona. Se implementa un flujo de trabajo completo que abarca la obtenci√≥n de datos desde la API de Idealista, la unificaci√≥n de datasets, el preprocesamiento exhaustivo y el entrenamiento de un modelo XGBoost. El artefacto principal generado es un pipeline serializado que encapsula todas las transformaciones de datos y el modelo, listo para ser utilizado en una aplicaci√≥n para realizar predicciones.

## üéØ Objetivos

* Obtener datos actualizados de viviendas en Barcelona desde la API de Idealista.
* Unificar los datos nuevos con los existentes, eliminando duplicados.
* Limpiar y preprocesar los datos para prepararlos para el modelado.
* Realizar una ingenier√≠a de caracter√≠sticas avanzada para extraer informaci√≥n relevante y crear nuevas variables.
* Entrenar un modelo de machine learning preciso para predecir los precios de las viviendas.
* Guardar el pipeline completo para facilitar su uso en producci√≥n.

---

## üöÄ Caracter√≠sticas del Proyecto

* **Obtenci√≥n de Datos de la API:** Extracci√≥n automatizada de datos de viviendas desde la API de Idealista.
* **Unificaci√≥n de Datasets:** Combinaci√≥n de nuevos datos con los existentes y eliminaci√≥n de duplicados.
* **Preprocesamiento Integral:** Limpieza, formateo, imputaci√≥n de nulos y transformaci√≥n de datos.
* **Ingenier√≠a de Caracter√≠sticas Avanzada:** Creaci√≥n de caracter√≠sticas espaciales, no lineales y de interacci√≥n.
* **Modelado Predictivo:** Entrenamiento y optimizaci√≥n de un modelo XGBoost Regressor.
* **Pipeline Serializado:** Guardado de todo el flujo de preprocesamiento y el modelo en un √∫nico archivo.

---

## üõ†Ô∏è Tecnolog√≠as y Herramientas Utilizadas

| Herramienta / Lenguaje | Descripci√≥n |
|------------------------|-------------|
| **Python** | Lenguaje principal para la manipulaci√≥n de datos, llamadas a la API y modelado. |
| **Pandas** | Librer√≠a para la manipulaci√≥n y an√°lisis de datos tabulares. |
| **NumPy** | Librer√≠a para la computaci√≥n num√©rica. |
| **Requests** | Librer√≠a para realizar peticiones HTTP a la API de Idealista. |
| **Base64** | Librer√≠a para la codificaci√≥n de credenciales de la API. |
| **Geopy** | Librer√≠a para calcular distancias geod√©sicas. |
| **Scikit-learn** | Librer√≠a para el preprocesamiento de datos, clustering, PCA y modelado. |
| **XGBoost** | Librer√≠a para el modelo de Gradient Boosting. |
| **Category Encoders** | Librer√≠a para la codificaci√≥n de variables categ√≥ricas. |
| **Joblib** | Librer√≠a para serializar y guardar el pipeline completo. |
| **Google Colab** | Entorno de desarrollo en la nube utilizado para ejecutar el c√≥digo. |

---

## üß† Metodolog√≠a General

1.  **Obtenci√≥n de Datos:** El notebook `1.IdealistaAPI.ipynb` se encarga de obtener datos de viviendas desde la API de Idealista. Se autentica, define los par√°metros de b√∫squeda y guarda los resultados en archivos CSV.
2.  **Unificaci√≥n de Datasets:** El notebook `2.AgregarNuevasViviendas.ipynb` combina los nuevos datos obtenidos de la API con los datos existentes, eliminando los registros duplicados.
3.  **Preprocesamiento Inicial:** El notebook `3.PreProcesamiento.ipynb` realiza una limpieza y transformaci√≥n inicial de los datos, eliminando columnas innecesarias y extrayendo informaci√≥n de columnas JSON.
4.  **Preprocesamiento Avanzado y Modelado:** El notebook `4.ModeloExport.ipynb` realiza el preprocesamiento avanzado, la ingenier√≠a de caracter√≠sticas y el entrenamiento del modelo XGBoost. El pipeline completo se guarda en un archivo `.joblib`.

---

## üìÅ Descripci√≥n de los Notebooks

* **`1.IdealistaAPI.ipynb`:**
    * Obtiene datos de la API de Idealista.
    * Realiza la autenticaci√≥n y las peticiones a la API.
    * Guarda los datos en archivos CSV.
    * **Para Ejecutar:**
        * Aseg√∫rate de tener instaladas las librer√≠as: `requests`, `base64`, `pandas`.
        * Reemplaza los valores de `API_KEY` y `API_SECRET` con tus propias credenciales de Idealista.
        * Ejecuta las celdas en orden. El CSV generado (`pisos_barcelona.csv`) contendr√° los datos obtenidos de la API.
* **`2.AgregarNuevasViviendas.ipynb`:**
    * Unifica los nuevos datos con los existentes.
    * Elimina los duplicados.
    * Guarda el dataset unificado.
    * **Para Ejecutar:**
        * Aseg√∫rate de tener instalada la librer√≠a `pandas`.
        * Modifica las rutas de los archivos CSV (`df1` y `df2`) para que apunten a tus archivos de datos.
        * Ejecuta las celdas en orden. El CSV generado (`pisos_barcelona_unificado.csv`) ser√° el dataset combinado y sin duplicados.
* **`3.PreProcesamiento.ipynb`:**
    * Realiza el preprocesamiento inicial de los datos.
    * Elimina columnas innecesarias.
    * Extrae informaci√≥n de columnas JSON.
    * **Para Ejecutar:**
        * Aseg√∫rate de tener instaladas las librer√≠as `pandas` y `ast`.
        * Modifica la ruta del archivo CSV para que apunte al dataset unificado (`pisos_barcelona_unificado.csv`).
        * Ejecuta las celdas en orden. El CSV generado (`pisosBarcelona-{fecha_actual}-clean.csv`) contendr√° los datos preprocesados.
* **`4.ModeloExport.ipynb`:**
    * Realiza el preprocesamiento avanzado.
    * Implementa la ingenier√≠a de caracter√≠sticas.
    * Entrena el modelo XGBoost.
    * Guarda el pipeline completo.
    * **Para Ejecutar:**
        * Este notebook requiere varias librer√≠as. La primera celda instala `category_encoders`. Aseg√∫rate de que todas est√©n instaladas (`pandas`, `numpy`, `geopy`, `scikit-learn`, `xgboost`, `joblib`).
        * Si est√°s utilizando Google Colab, aseg√∫rate de que tu Google Drive est√© montado correctamente (la primera celda del notebook lo hace). Modifica la ruta del archivo CSV si es necesario.
        * Ejecuta las celdas en orden. El archivo generado (`pipeline_idealista_completo.joblib`) contendr√° el pipeline completo y estar√° guardado en la ruta especificada.

---

**Consideraciones Importantes:**

* **Entorno de Ejecuci√≥n:** Los notebooks est√°n dise√±ados para ser ejecutados en Google Colab, pero tambi√©n pueden funcionar en un entorno Jupyter local si se configuran las rutas de los archivos y se instalan las librer√≠as necesarias.
* **Dependencias:** Aseg√∫rate de instalar todas las librer√≠as requeridas antes de ejecutar cada notebook. Puedes usar `pip install <libreria>` o ejecutar la primera celda del notebook `4.ModeloExport.ipynb` para instalar `category_encoders`.
* **Rutas de Archivos:** Ajusta las rutas de los archivos CSV para que coincidan con la ubicaci√≥n de tus datos.
* **Credenciales de la API:** En el notebook `1.IdealistaAPI.ipynb`, debes proporcionar tus propias credenciales de la API de Idealista (API\_KEY y API\_SECRET).
* **Orden de Ejecuci√≥n:** Es importante ejecutar los notebooks en el orden num√©rico para asegurar que los datos se procesen correctamente en cada etapa.

Este README proporciona una visi√≥n general del proyecto, su prop√≥sito, la metodolog√≠a seguida, las herramientas utilizadas y una gu√≠a para ejecutar los notebooks. Los notebooks detallan cada etapa del proceso, desde la obtenci√≥n de los datos hasta la generaci√≥n del modelo predictivo.
