# üè† Pron√≥stico de Precios de Vivienda en Barcelona: An√°lisis y Predicci√≥n con Datos de Idealista

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Jupyter Notebook](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)](https://jupyter.org/)
[![Pandas](https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![Scikit-Learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)
[![Power BI](https://img.shields.io/badge/Power_BI-F2C811?style=for-the-badge&logo=powerbi&logoColor=black)](https://powerbi.microsoft.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)](https://www.r-project.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE) [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/tu_usuario/tu_repositorio/graphs/commit-activity) ## üéØ Descripci√≥n del Proyecto

Este repositorio contiene un proyecto integral para el **an√°lisis y pron√≥stico de datos de vivienda en Barcelona**, con un enfoque particular en los datos obtenidos de Idealista. El proyecto abarca todo el ciclo de vida de un proyecto de ciencia de datos:

* **Adquisici√≥n de datos**: Obtenci√≥n de datos directamente de la API de Idealista.
* **Preprocesamiento de datos**: Limpieza, transformaci√≥n y preparaci√≥n de los datos para el modelado.
* **Modelado predictivo**: Construcci√≥n y evaluaci√≥n de modelos de machine learning para predecir los precios de la vivienda.
* **Visualizaci√≥n de resultados**: Creaci√≥n de visualizaciones interactivas para comunicar los hallazgos y los resultados del modelo.

Se utilizan diversas herramientas y tecnolog√≠as, destacando el uso extensivo de **Jupyter Notebooks** para la manipulaci√≥n, an√°lisis y modelado de datos.

## üìÇ Estructura del Repositorio

A continuaci√≥n, se describe la estructura del repositorio y el prop√≥sito de cada componente:

    ‚îú‚îÄ‚îÄ Documentaci√≥n/
    ‚îÇ   ‚îú‚îÄ‚îÄ memoria.md
    ‚îÇ   ‚îú‚îÄ‚îÄ readme.md
    ‚îÇ   ‚îî‚îÄ‚îÄ workflow.md
    ‚îú‚îÄ‚îÄ Datos_y_Modelos/
    ‚îÇ   ‚îú‚îÄ‚îÄ BarcelonaHousingForecastBI.pbix
    ‚îÇ   ‚îî‚îÄ‚îÄ Modelos/
    ‚îÇ       ‚îî‚îÄ‚îÄ pipeline_idealista_completo.joblib
    ‚îú‚îÄ‚îÄ PostgreSQL/
    ‚îÇ   ‚îú‚îÄ‚îÄ CrearTabla.sql
    ‚îÇ   ‚îú‚îÄ‚îÄ PasarDatosCSV.sql
    ‚îÇ   ‚îú‚îÄ‚îÄ CrearIndiceEspacial.sql
    ‚îÇ   ‚îî‚îÄ‚îÄ AjustarValores.sql
    ‚îî‚îÄ‚îÄ Scripts/
        ‚îú‚îÄ‚îÄ 1.IdealistaAPI.ipynb
        ‚îú‚îÄ‚îÄ 2.AgregarNuevasViviendas.ipynb
        ‚îú‚îÄ‚îÄ 3.PreProcesamiento.ipynb
        ‚îú‚îÄ‚îÄ 4.ModeloExport.ipynb
        ‚îú‚îÄ‚îÄ Aux.EDA.ipynb
        ‚îú‚îÄ‚îÄ Aux.IdealistaModelo.ipynb
        ‚îú‚îÄ‚îÄ Aux.IdealistaModeloAmpliado.ipynb
        ‚îú‚îÄ‚îÄ Aux.ModeloImport.ipynb
        ‚îî‚îÄ‚îÄ Graficaciones.R
    ‚îú‚îÄ‚îÄ .vscode/
    ‚îÇ   ‚îî‚îÄ‚îÄ settings.json
    ‚îî‚îÄ‚îÄ Orange/
        ‚îî‚îÄ‚îÄ OrangeWorkflow.ows
    ‚îî‚îÄ‚îÄ LICENSE
    ‚îî‚îÄ‚îÄ README.md

## üóÇÔ∏è Componentes Clave

### 1. Documentaci√≥n

* **`memoria.md`**: Documento que contiene la memoria del proyecto, incluyendo la justificaci√≥n, metodolog√≠a, resultados y conclusiones.
* **`readme.md`**: El archivo README principal del repositorio (este archivo).
* **`workflow.md`**: Descripci√≥n del flujo de trabajo del proyecto, incluyendo los pasos secuenciales y las dependencias entre las diferentes etapas.

### 2. Datos y Modelos

* **`BarcelonaHousingForecastBI.pbix`**: Archivo de Power BI que contiene cuadros de mando (dashboards) e informes interactivos para visualizar los resultados del an√°lisis y pron√≥stico.
* **`Modelos/pipeline_idealista_completo.joblib`**: Archivo que guarda un pipeline completo de machine learning, serializado con la librer√≠a `joblib`. Este pipeline incluye pasos de preprocesamiento y el modelo predictivo entrenado.

### 3. Base de Datos (PostgreSQL)

* **`PostgreSQL/CrearTabla.sql`**: Script SQL para definir la estructura de las tablas en la base de datos PostgreSQL.
* **`PostgreSQL/PasarDatosCSV.sql`**: Script SQL para cargar datos desde archivos CSV a las tablas de la base de datos.
* **`PostgreSQL/CrearIndiceEspacial.sql`**: Script SQL para crear √≠ndices espaciales en la base de datos, optimizando las consultas basadas en la ubicaci√≥n.
* **`PostgreSQL/AjustarValores.sql`**: Script SQL para realizar ajustes o transformaciones en los valores de la base de datos.

### 4. Scripts

#### 4.1 Scripts de Python (Jupyter Notebooks)

* **`Scripts/1.IdealistaAPI.ipynb`**: Notebook para la adquisici√≥n de datos directamente desde la API de Idealista.

    * **Funcionalidad**: Obtenci√≥n de datos de anuncios de vivienda desde la API de Idealista.
    * **Detalles**:
        * Llamadas a la API de Idealista para obtener datos de viviendas.
        * Manejo de autenticaci√≥n, respuestas JSON y paginaci√≥n.
        * Posible manejo de errores de la API.
        * Ejemplo de uso de la API:

            ```python
            import requests

            url = "[https://api.idealista.com/3.5/es/search](https://api.idealista.com/3.5/es/search)"
            params = {
                "operation": "sale",
                "locationId": "0-1",  # Ejemplo: Barcelona
                "maxItems": 50,
                "numPage": 1
            }
            headers = {
                "Authorization": "Bearer TU_TOKEN_DE_AUTORIZACION" # Reemplazar
            }

            response = requests.get(url, params=params, headers=headers)

            if response.status_code == 200:
                data = response.json()
                print(data)
            else:
                print(f"Error al obtener datos: {response.status_code}")
            ```

* **`Scripts/2.AgregarNuevasViviendas.ipynb`**: Notebook para incorporar nuevas entradas de vivienda a un conjunto de datos existente.

    * **Funcionalidad**: Incorporaci√≥n de nuevos datos de viviendas a un conjunto de datos maestro.
    * **Detalles**:
        * Integraci√≥n de nuevos datos (de la API o de otra fuente) en un conjunto de datos existente.
        * Manejo de duplicados e inconsistencias.
        * Posible uso de `pandas`:

            ```python
            import pandas as pd

            # Cargar datos existentes
            df_existente = pd.read_csv("datos_existentes.csv")

            # Crear un DataFrame con los nuevos datos
            nuevos_datos = {
                "id": [101, 102],
                "precio": [250000, 300000],
                "ubicacion": ["Centro", "Gracia"]
            }
            df_nuevos = pd.DataFrame(nuevos_datos)

            # Concatenar los DataFrames
            df_combinado = pd.concat([df_existente, df_nuevos], ignore_index=True)

            # Eliminar duplicados
            df_combinado = df_combinado.drop_duplicates()

            print(df_combinado)
            df_combinado.to_csv("datos_actualizados.csv", index=False)
            ```

* **`Scripts/3.PreProcesamiento.ipynb`**: Notebook para la limpieza y transformaci√≥n de los datos.

    * **Funcionalidad**: Limpieza y preparaci√≥n de los datos para el modelado.
    * **Detalles**:
        * Manejo de valores faltantes (imputaci√≥n, eliminaci√≥n).
        * Detecci√≥n y tratamiento de valores at√≠picos (outliers).
        * Codificaci√≥n de variables categ√≥ricas (one-hot encoding, label encoding).
        * Escalado de variables num√©ricas (normalizaci√≥n, estandarizaci√≥n).
        * Ingenier√≠a de caracter√≠sticas (creaci√≥n de nuevas caracter√≠sticas).
        * Ejemplo de preprocesamiento con `scikit-learn`:

            ```python
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import StandardScaler, OneHotEncoder
            from sklearn.impute import SimpleImputer
            from sklearn.compose import ColumnTransformer
            from sklearn.pipeline import Pipeline

            # Cargar datos
            df = pd.read_csv("datos_vivienda.csv")

            # Separar variable objetivo
            X = df.drop("precio", axis=1)
            y = df["precio"]

            # Identificar columnas num√©ricas y categ√≥ricas
            numeric_features = X.select_dtypes(include=['number']).columns
            categorical_features = X.select_dtypes(include=['object']).columns

            # Crear pipeline de preprocesamiento para variables num√©ricas
            numeric_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ])

            # Crear pipeline de preprocesamiento para variables categ√≥ricas
            categorical_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ])

            # Combinar ambos pipelines usando ColumnTransformer
            preprocessor = ColumnTransformer(
                transformers=[
                    ('num', numeric_transformer, numeric_features),
                    ('cat', categorical_transformer, categorical_features)
                ])
            # Dividir los datos en entrenamiento y prueba
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # Aplicar el preprocesamiento a los datos de entrenamiento y prueba
            X_train_transformed = preprocessor.fit_transform(X_train)
            X_test_transformed = preprocessor.transform(X_test)

            print("X_train_transformed shape:", X_train_transformed.shape)
            print("X_test_transformed shape:", X_test_transformed.shape)
            ```

* **`Scripts/4.ModeloExport.ipynb`**: Notebook para exportar y guardar el modelo predictivo entrenado.

    * **Funcionalidad**: Guardar el modelo entrenado para su uso posterior.
    * **Detalles**:
        * Serializaci√≥n del modelo y del pipeline de preprocesamiento usando `joblib`.
        * Ejemplo de exportaci√≥n:

            ```python
            from joblib import dump

            # Asumiendo que 'modelo' es tu modelo entrenado
            # y 'preprocessor' es el ColumnTransformer del paso anterior
            pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', modelo)]) # Crear pipeline

            dump(pipeline, 'Modelos/pipeline_idealista_completo.joblib')
            print("Modelo exportado a Modelos/pipeline_idealista_completo.joblib")
            ```

* **`Scripts/Aux.EDA.ipynb`**: Notebook auxiliar para realizar An√°lisis Exploratorio de Datos (EDA).

    * **Funcionalidad**: An√°lisis exploratorio de los datos para comprender sus caracter√≠sticas.
    * **Detalles**:
        * Visualizaci√≥n de la distribuci√≥n de los datos.
        * Identificaci√≥n de relaciones entre variables.
        * Detecci√≥n de patrones y anomal√≠as.
        * Uso de librer√≠as como `pandas`, `numpy`, `matplotlib`, `seaborn` y `plotly`.
        * Ejemplo de EDA:

            ```python
            import pandas as pd
            import matplotlib.pyplot as plt
            import seaborn as sns

            # Cargar datos
            df = pd.read_csv("datos_vivienda.csv")

            # Mostrar las primeras filas del DataFrame
            print(df.head())

            # Obtener informaci√≥n sobre los tipos de datos y valores no nulos
            print(df.info())

            # Resumen estad√≠stico de las variables num√©ricas
            print(df.describe())

            # Histograma de la variable 'precio'
            plt.figure(figsize=(8, 6))
            sns.histplot(df['precio'], kde=True)
            plt.title('Distribuci√≥n del Precio de la Vivienda')
            plt.xlabel('Precio')
            plt.ylabel('Frecuencia')
            plt.show()

            # Gr√°fico de dispersi√≥n entre 'tama√±o' y 'precio'
            plt.figure(figsize=(8, 6))
            sns.scatterplot(x='tama√±o', y='precio', data=df)
            plt.title('Relaci√≥n entre Tama√±o y Precio')
            plt.xlabel('Tama√±o (m2)')
            plt.ylabel('Precio')
            plt.show()

            # Boxplot de 'precio' por 'ubicacion'
            plt.figure(figsize=(10, 6))
            sns.boxplot(x='ubicacion', y='precio', data=df)
            plt.title('Distribuci√≥n del Precio por Ubicaci√≥n')
            plt.xlabel('Ubicacion')
            plt.ylabel('Precio')
            plt.show()
            ```

* **`Scripts/Aux.IdealistaModelo.ipynb`**: Notebook auxiliar para la construcci√≥n y entrenamiento de un modelo predictivo utilizando los datos de Idealista.

    * **Funcionalidad**: Construcci√≥n de un modelo de machine learning para predecir los precios de la vivienda.
    * **Detalles**:
        * Selecci√≥n de caracter√≠sticas.
        * Divisi√≥n de datos en conjuntos de entrenamiento y prueba.
        * Selecci√≥n y configuraci√≥n de un modelo de regresi√≥n.
        * Entrenamiento del modelo.
        * Evaluaci√≥n del rendimiento del modelo (RMSE, MAE, R-squared).
        * Ejemplo de modelado con `scikit-learn`:

            ```python
            from sklearn.model_selection import train_test_split
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

            # Dividir los datos en entrenamiento y prueba
            X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

            # Crear el modelo de regresi√≥n lineal
            modelo = LinearRegression()

            # Entrenar el modelo
            modelo.fit(X_train, y_train)

            # Hacer predicciones en el conjunto de prueba
            y_pred = modelo.predict(X_test)

            # Evaluar el rendimiento del modelo
            rmse = mean_squared_error(y_test, y_pred, squared=False)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            print(f"RMSE: {rmse}")
            print(f"MAE: {mae}")
            print(f"R-squared: {r2}")
            ```

* **`Scripts/Aux.IdealistaModeloAmpliado.ipynb`**: Notebook auxiliar para explorar modelos m√°s complejos o enfoques alternativos.

    * **Funcionalidad**: Experimentaci√≥n con diferentes modelos y t√©cnicas para mejorar la precisi√≥n del pron√≥stico.
    * **Detalles**:
        * Exploraci√≥n de otros algoritmos de machine learning (e.g., Random Forest, Gradient Boosting).
        * Optimizaci√≥n de hiperpar√°metros (e.g., Grid Search, Random Search).
        * Uso de t√©cnicas de ensemble.
        * Incorporaci√≥n de caracter√≠sticas adicionales.
        * Ejemplo de Grid Search para optimizaci√≥n de hiperpar√°metros:

            ```python
            from sklearn.model_selection import GridSearchCV
            from sklearn.ensemble import RandomForestRegressor

            # Definir la cuadr√≠cula de hiperpar√°metros a explorar
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [5, 10, 15],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }

            # Crear el modelo de Random Forest
            rf_model = RandomForestRegressor(random_state=42)

            # Crear el objeto GridSearchCV
            grid_search = GridSearchCV(estimator=rf_model,
                                            param_grid=param_grid,
                                            cv=5,  # Validaci√≥n cruzada de 5 pliegues
                                            scoring='neg_mean_squared_error',  # M√©trica a optimizar
                                            verbose=1,
                                            n_jobs=-1)  # Usar todos los procesadores disponibles

            # Realizar la b√∫squeda de la mejor combinaci√≥n de hiperpar√°metros
            grid_search.fit(X_train, y_train)

            # Imprimir los mejores hiperpar√°metros encontrados
            print("Mejores hiperpar√°metros:", grid_search.best_params_)

            # Obtener el mejor modelo
            best_model = grid_search.best_estimator_

            # Evaluar el rendimiento del mejor modelo en el conjunto de prueba
            y_pred_best = best_model.predict(X_test)
            rmse_best = mean_squared_error(y_test, y_pred_best, squared=False)
            print(f"RMSE del mejor modelo: {rmse_best}")
            ```

* **`Scripts/Aux.ModeloImport.ipynb`**: Notebook auxiliar para cargar un modelo previamente entrenado y guardado.

    * **Funcionalidad**: Cargar un modelo previamente entrenado para realizar predicciones.
    * **Detalles**:
        * Carga del archivo `.joblib` que contiene el modelo y el pipeline.
        * Ejemplo de importaci√≥n y uso del modelo:

            ```python
            from joblib import load

            # Cargar el modelo desde el archivo
            pipeline_cargado = load('Modelos/pipeline_idealista_completo.joblib')

            # Asumiendo que tienes nuevos datos de entrada para hacer predicciones
            nuevos_datos = pd.DataFrame({
                'tama√±o': [100, 120, 80],
                'habitaciones': [3, 4, 2],
                'ubicacion': ['Centro', 'Gracia', 'Eixample']
            })  # Crear un DataFrame con los nuevos datos

            # Realizar predicciones con los nuevos datos
            predicciones = pipeline_cargado.predict(nuevos_datos)

            print("Predicciones:", predicciones)
            ```

#### 4.2 Script de R

* **`Scripts/Graficaciones.R`**: Script en lenguaje R para la generaci√≥n de visualizaciones.

    * **Funcionalidad**: Creaci√≥n de visualizaciones para comunicar los hallazgos del proyecto.
    * **Detalles**:
        * Generaci√≥n de gr√°ficos complejos y mapas tem√°ticos.
        * Uso de librer√≠as de R como `ggplot2`.
        * Ejemplo de visualizaci√≥n con `ggplot2`:

            ```R
            library(ggplot2)

            # Crear un data frame de ejemplo
            datos <- data.frame(
                ubicacion = c("Centro", "Gracia", "Eixample", "Sarri√†-Sant Gervasi"),
                precio_promedio = c(350000, 300000, 400000, 450000),
                error_estandar = c(20000, 15000, 25000, 30000)
            )

            # Crear un gr√°fico de barras con barras de error
            ggplot(datos, aes(x = ubicacion, y = precio_promedio)) +
                geom_bar(stat = "identity", fill = "steelblue") +
                geom_errorbar(aes(ymin = precio_promedio - error_estandar,
                                  ymax = precio_promedio + error_estandar),
                              width = 0.2) +
                labs(title = "Precio Promedio de la Vivienda por Ubicaci√≥n",
                     x = "Ubicaci√≥n",
                     y = "Precio Promedio") +
                theme_minimal()
            ```

### 5. Otros Archivos

* **.vscode/settings.json**: Archivo de configuraci√≥n para el editor Visual Studio Code.
* **Orange/OrangeWorkflow.ows**: Replicaci√≥n del modelo generado para las predicciones.

## üìà Flujo de Trabajo del Proyecto

El flujo de trabajo general del proyecto se puede resumir en los siguientes pasos:

1.  **Adquisici√≥n de Datos**: Obtenci√≥n de datos de vivienda de la API de Idealista utilizando `Scripts/1.IdealistaAPI.ipynb`.
2.  **Incorporaci√≥n de Datos**: Integraci√≥n de nuevos datos con los datos existentes utilizando `Scripts/2.AgregarNuevasViviendas.ipynb`.
3.  **Carga a la Base de Datos**: Carga de datos en una base de datos PostgreSQL, incluyendo la creaci√≥n de la estructura de la base de datos y la carga de datos desde archivos CSV.
4.  **An√°lisis Exploratorio de Datos (EDA)**: Realizaci√≥n de un EDA para comprender las caracter√≠sticas de los datos utilizando `Scripts/Aux.EDA.ipynb`.
5.  **Preprocesamiento de Datos**: Limpieza y transformaci√≥n de los datos utilizando `Scripts/3.PreProcesamiento.ipynb`.
6.  **Modelado**: Construcci√≥n y entrenamiento de modelos predictivos utilizando `Scripts/Aux.IdealistaModelo.ipynb` y `Scripts/Aux.IdealistaModeloAmpliado.ipynb`.
7.  **Exportaci√≥n del Modelo**: Guardado del modelo entrenado utilizando `Scripts/4.ModeloExport.ipynb`.
8.  **Importaci√≥n del Modelo**: Carga del modelo guardado para realizar predicciones utilizando `Scripts/Aux.ModeloImport.ipynb`.
9.  **Visualizaci√≥n**: Creaci√≥n de visualizaciones para comunicar los resultados del proyecto utilizando `Scripts/Graficaciones.R` y Power BI.
10. **Documentaci√≥n**: Documentaci√≥n del proyecto en `memoria.md` y `workflow.md`.

## ü§ù Contribuci√≥n

¬°Las contribuciones son bienvenidas! Si deseas contribuir a este proyecto, por favor, sigue estos pasos:

1.  Haz un fork del repositorio.
2.  Crea una rama para tu contribuci√≥n (`git checkout -b mi-contribucion`).
3.  Realiza tus cambios.
4.  Haz commit de tus cambios (`git commit -m "A√±ade una nueva caracter√≠stica"`).
5.  Sube tus cambios a tu fork (`git push origin mi-contribucion`).
6.  Crea un pull request.

## üìÑ Licencia

Este proyecto est√° licenciado bajo la licencia [MIT](LICENSE).

## üìß Contacto

Si tienes alguna pregunta o sugerencia, no dudes en ponerte en contacto conmigo a trav√©s de [tu_correo@example.com](glayolacvs@gmail.com).
