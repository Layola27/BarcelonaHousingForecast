# üèòÔ∏è Memoria T√©cnica: Plataforma Web de An√°lisis y Predicci√≥n de Precios de Viviendas en Barcelona

## 1. Introducci√≥n

Este documento presenta la memoria t√©cnica detallada de una plataforma web full-stack desarrollada para el an√°lisis y la predicci√≥n de precios de viviendas en Barcelona y sus alrededores. La aplicaci√≥n ofrece tres funcionalidades principales: la estimaci√≥n del precio de una vivienda a partir de sus caracter√≠sticas, la consulta interactiva del conjunto de datos mediante lenguaje natural y la solicitud de un informe resumido generado por inteligencia artificial enviado por correo electr√≥nico. El objetivo de esta memoria es documentar exhaustivamente la arquitectura, los componentes t√©cnicos, las funcionalidades implementadas y las tecnolog√≠as utilizadas en el desarrollo de esta plataforma.

## 2. Arquitectura General

La arquitectura de la plataforma se basa en un dise√±o modular y distribuido, facilitado por la contenerizaci√≥n con Docker. Los principales componentes que interact√∫an para ofrecer la funcionalidad completa son:

* **Frontend (React):** Interfaz de usuario interactiva que permite a los usuarios interactuar con las diferentes funcionalidades de la plataforma.
* **Backend API (FastAPI):** API RESTful que act√∫a como intermediario entre el frontend y los dem√°s servicios, gestionando las peticiones de predicci√≥n, consulta de datos y generaci√≥n de informes.
* **Base de Datos (PostgreSQL):** Almacenamiento persistente del dataset de viviendas de Barcelona.
* **Servicio LLM (Ollama):** Ejecuci√≥n local de modelos de lenguaje para la consulta en lenguaje natural de la base de datos.
* **Automatizaci√≥n de Informes (n8n):** Plataforma de automatizaci√≥n para la generaci√≥n y env√≠o de informes resumidos por correo electr√≥nico.

La comunicaci√≥n entre estos componentes se realiza principalmente a trav√©s de peticiones HTTP, facilitando la escalabilidad y el mantenimiento del sistema.

## 3. Base de Datos (PostgreSQL)

### 3.1. Descripci√≥n

Se utiliza una base de datos PostgreSQL para almacenar y gestionar el dataset de viviendas de Barcelona. Esta elecci√≥n se basa en su robustez, fiabilidad y capacidad para manejar grandes vol√∫menes de datos estructurados.

### 3.2. Esquema de la Base de Datos

* **Motor:** PostgreSQL
* **Contenedor:** Docker
* **Nombre de la Base de Datos:** `housing_db`
* **Schema:** `public`
* **Tabla Principal:** `pisos_barcelona` (o `viviendas`)

### 3.3. Datos

El dataset inicial se carg√≥ desde un archivo CSV (`pisosBarcelona-21-04-2025-clean.csv`) que conten√≠a aproximadamente 8400 registros y 15MB de informaci√≥n. Cada registro representa una vivienda y contiene campos detallados sobre su precio, tama√±o, n√∫mero de habitaciones y ba√±os, ubicaci√≥n (latitud, longitud), caracter√≠sticas adicionales (ascensor, parking, etc.) y otros metadatos relevantes.

## 4. Modelo Predictivo de Precios

### 4.1. Descripci√≥n

Se implement√≥ un modelo de Machine Learning para estimar el precio de las viviendas bas√°ndose en sus caracter√≠sticas. El objetivo es proporcionar una herramienta precisa para la valoraci√≥n inmobiliaria.

### 4.2. Modelo Utilizado

* **Algoritmo:** XGBoost Regressor

### 4.3. Ingenier√≠a de Caracter√≠sticas (Feature Engineering)

Se aplic√≥ un extenso proceso de ingenier√≠a de caracter√≠sticas para optimizar el rendimiento del modelo. Los pasos clave incluyeron:

* **Limpieza de Datos:** Manejo de valores inconsistentes (ej., pisos 'bj'/'en' como plantas bajas), tratamiento de valores 'Unknown'.
* **Imputaci√≥n de Valores Nulos:** Reemplazo de valores faltantes utilizando la mediana de la columna correspondiente. Se crearon flags (`isna_`) para indicar la presencia original de valores nulos.
* **Combinaci√≥n de Features:** Creaci√≥n de nuevas caracter√≠sticas a partir de las existentes (ej., `parking_status`).
* **Ingenier√≠a de Caracter√≠sticas Espaciales:**
    * C√°lculo de la distancia a Puntos de Inter√©s (POIs).
    * Aplicaci√≥n de clustering KMeans sobre las coordenadas geogr√°ficas para identificar zonas.
    * Aplicaci√≥n de An√°lisis de Componentes Principales (PCA) sobre las coordenadas para reducir la dimensionalidad y capturar la variabilidad espacial.
* **Codificaci√≥n de Variables Categ√≥ricas:** Utilizaci√≥n de TargetEncoder para las columnas `propertyType`, `status` y `parking_status`.
* **Creaci√≥n de Features No Lineales:**
    * Generaci√≥n de Splines sobre las variables num√©ricas `size` y las distancias a POIs para capturar relaciones no lineales.
    * Creaci√≥n de t√©rminos de interacci√≥n polin√≥mica de grado 2 entre las caracter√≠sticas.
* **Transformaci√≥n del Target:** Aplicaci√≥n de la transformaci√≥n logar√≠tmica (`log1p`) a la variable objetivo (`price`) para normalizar su distribuci√≥n.
* **Filtrado de Outliers:** Eliminaci√≥n de valores at√≠picos basados en los percentiles 1 y 99 de la variable objetivo.
* **Escalado:** Aplicaci√≥n de `StandardScaler` a las caracter√≠sticas num√©ricas para asegurar que tengan una escala similar antes de alimentar el modelo.

### 4.4. Persistencia del Modelo

Todo el pipeline de preprocesamiento y el modelo XGBoost entrenado se serializaron y guardaron en un √∫nico archivo utilizando la librer√≠a `joblib`:

* **Archivo:** `pipeline_idealista_completo.joblib`

Este archivo permite cargar el modelo y el pipeline completo de transformaci√≥n de datos de manera eficiente para realizar predicciones en tiempo real.

## 5. Backend API (FastAPI)

### 5.1. Descripci√≥n

Se desarroll√≥ una API RESTful utilizando el framework FastAPI (basado en Python) para exponer las funcionalidades del modelo predictivo, la consulta de la base de datos y la generaci√≥n de informes. Uvicorn se utiliza como servidor ASGI para ejecutar la aplicaci√≥n FastAPI.

### 5.2. Endpoints

* **`/predict/` (POST):**
    * **Prop√≥sito:** Recibe las caracter√≠sticas de una vivienda en formato JSON seg√∫n el modelo Pydantic `HouseFeaturesPredict`.
    * **Proceso:** Carga el pipeline serializado desde `pipeline_idealista_completo.joblib`, aplica las transformaciones necesarias a los datos de entrada y utiliza el modelo XGBoost para predecir el precio. La predicci√≥n se devuelve deshaciendo la transformaci√≥n logar√≠tmica aplicada durante el entrenamiento.
    * **Respuesta:** Un JSON con la predicci√≥n del precio.
* **`/query_database/` (POST):**
    * **Prop√≥sito:** Recibe una pregunta en lenguaje natural del usuario en formato JSON seg√∫n el modelo Pydantic `QueryRequest`.
    * **Proceso:** Utiliza el agente LangChain SQL configurado (ver secci√≥n 6) para procesar la pregunta, generar una consulta SQL, ejecutarla en la base de datos PostgreSQL y obtener una respuesta textual del modelo LLM.
    * **Respuesta:** Un JSON con la respuesta del modelo de lenguaje.
* **`/generate_simple_report/` (POST):**
    * **Prop√≥sito:** Act√∫a como un intermediario para iniciar el workflow de generaci√≥n de informes en n8n.
    * **Proceso:** Realiza una petici√≥n HTTP (webhook) al endpoint configurado en el servicio n8n. Puede recibir opcionalmente un correo electr√≥nico como par√°metro.
    * **Respuesta:** Un JSON indicando el estado de la solicitud.

### 5.3. Configuraci√≥n Adicional

* **CORS (Cross-Origin Resource Sharing):** Se configur√≥ el middleware de CORS para permitir las peticiones provenientes del frontend React, que se ejecuta en un dominio diferente.

## 6. Consulta con Lenguaje Natural (LLM + LangChain + Ollama)

### 6.1. Descripci√≥n

Se implement√≥ una funcionalidad para permitir a los usuarios consultar la base de datos de viviendas utilizando lenguaje natural. Esto se logra mediante la integraci√≥n de un modelo de lenguaje grande (LLM) ejecutado localmente con Ollama y la librer√≠a LangChain para la orquestaci√≥n.

### 6.2. Tecnolog√≠as Utilizadas

* **Ollama:** Un servicio en Docker que permite ejecutar modelos LLM de forma local. Se probaron los modelos `llama3:8b` y `mixtral:8x7b-instruct-v0.1-q4_K_M`.
* **LangChain:** Una librer√≠a para construir aplicaciones basadas en LLMs. Se utilizaron los siguientes m√≥dulos:
    * `ChatOllama`: Para la comunicaci√≥n con el modelo LLM ejecutado por Ollama.
    * `SQLDatabase`: Para establecer la conexi√≥n con la base de datos PostgreSQL y obtener el esquema de la tabla `pisos_barcelona`.
    * `SQLDatabaseToolkit`: Un conjunto de herramientas para interactuar con la base de datos SQL.
    * `create_sql_agent`: Una funci√≥n para crear un agente que puede traducir preguntas en lenguaje natural a consultas SQL, ejecutarlas y formular una respuesta. Se configur√≥ con `agent_type="openai-tools"`.

### 6.3. Flujo de la Consulta

1.  El usuario introduce una pregunta en lenguaje natural a trav√©s de la interfaz de chat en el frontend.
2.  El frontend env√≠a esta pregunta al endpoint `/query_database/` del backend.
3.  El backend recibe la pregunta y la pasa al agente LangChain SQL.
4.  El agente utiliza el modelo LLM (a trav√©s de `ChatOllama`) y el esquema de la base de datos (a trav√©s de `SQLDatabaseToolkit`) para generar una consulta SQL relevante para la pregunta del usuario.
5.  La consulta SQL se ejecuta en la base de datos PostgreSQL (a trav√©s de la conexi√≥n definida en `SQLDatabase`).
6.  Los resultados de la consulta se devuelven al modelo LLM.
7.  El modelo LLM formula una respuesta en lenguaje natural basada en los resultados de la consulta. Se personaliz√≥ el `suffix` del prompt del agente para asegurar respuestas concisas y en espa√±ol.
8.  La respuesta del LLM se env√≠a de vuelta al frontend y se muestra al usuario en la interfaz de chat.

## 7. Automatizaci√≥n de Informes (n8n)

### 7.1. Descripci√≥n

Se implement√≥ un workflow de automatizaci√≥n utilizando la plataforma n8n para generar informes resumidos sobre los datos de viviendas y enviarlos por correo electr√≥nico a los usuarios que lo soliciten.

### 7.2. Configuraci√≥n de n8n

* **Contenedor:** Docker
* **Trigger:** Nodo Webhook configurado para ser llamado por el endpoint `/generate_simple_report/` del backend (o directamente desde el frontend).

### 7.3. Pasos del Workflow

1.  **Webhook:** Recibe la petici√≥n del backend (o frontend), que puede incluir una direcci√≥n de correo electr√≥nico.
2.  **Consulta a PostgreSQL:** Utiliza el nodo PostgreSQL de n8n para ejecutar consultas SQL predefinidas o din√°micas en la base de datos `pisos_barcelona`. Estas consultas se dise√±an para obtener datos agregados o relevantes para el informe (ej., precio medio por distrito, distribuci√≥n de tipos de vivienda, etc.).
3.  **Formateo de Datos (Function Node):** Un nodo Function de JavaScript se utiliza para procesar y formatear los datos obtenidos de la base de datos en una estructura adecuada para ser utilizada por el LLM.
4.  **Generaci√≥n del Informe con LLM (HTTP Request / OpenAI Node):** Se realiza una llamada a una API LLM externa (Google Gemini o OpenAI) utilizando un nodo HTTP Request o el nodo espec√≠fico de la plataforma LLM. Se pasa un prompt detallado junto con los datos formateados para instruir al LLM a generar un informe conciso en formato Markdown y en espa√±ol, adecuado para ser incluido en un correo electr√≥nico.
5.  **Env√≠o de Correo Electr√≥nico (Send Email Node):** Utiliza el nodo "Send Email" de n8n, configurado con una cuenta de Gmail (v√≠a OAuth2) o mediante SMTP con una contrase√±a de aplicaci√≥n, para enviar el informe generado a la direcci√≥n de correo electr√≥nico proporcionada en la petici√≥n inicial (o a una direcci√≥n por defecto si no se proporciona).

## 8. Frontend (React)

### 8.1. Descripci√≥n

La interfaz de usuario de la plataforma se desarroll√≥ como una Single Page Application (SPA) utilizando la librer√≠a React y el bundler Vite. Se prioriz√≥ la interactividad y una experiencia de usuario intuitiva.

### 8.2. Librer√≠as UI

* **Material UI (MUI):** Se utiliz√≥ extensivamente para proporcionar componentes de interfaz de usuario estilizados y responsivos (Container, Paper, Grid, TextField, Button, Select, Checkbox, Accordion, Dialog, Alert, etc.).

### 8.3. Mapa Interactivo (React Leaflet)

* Se integr√≥ un mapa interactivo utilizando la librer√≠a React Leaflet, centrado en la ciudad de Barcelona.
* Se visualiza un c√≠rculo con un radio de 15 km alrededor del centro de Barcelona para indicar el √°rea de validez del modelo predictivo.
* Los usuarios pueden hacer clic dentro de este c√≠rculo para colocar un marcador, lo que actualiza autom√°ticamente los campos de latitud y longitud en el formulario del estimador de precios.
* Se implement√≥ una restricci√≥n para evitar la selecci√≥n de puntos fuera del c√≠rculo, mostrando una advertencia visual cuando se intenta.

### 8.4. Secciones de la Aplicaci√≥n

* **Estimador de Precios:**
    * Un formulario organizado mediante componentes `Accordion` de MUI permite a los usuarios introducir las caracter√≠sticas de una vivienda (tama√±o, habitaciones, ba√±os, ubicaci√≥n en el mapa, caracter√≠sticas adicionales como ascensor y parking, etc.).
    * Los campos de latitud y longitud se actualizan autom√°ticamente al interactuar con el mapa.
    * Al enviar el formulario, se realiza una petici√≥n al endpoint `/predict/` del backend.
    * La interfaz muestra el precio estimado o mensajes de error en caso de problemas con la predicci√≥n.
* **Consulta con IA:**
    * Un bot√≥n con un icono de bot (`SmartToyIcon`) abre un componente `Dialog` modal.
    * Dentro del di√°logo, se presenta una interfaz de chat donde los usuarios pueden escribir sus preguntas en lenguaje natural.
    * Las preguntas se env√≠an al endpoint `/query_database/` del backend.
    * Las respuestas del agente LLM se muestran en la conversaci√≥n dentro del di√°logo.
* **Solicitud de Informe:**
    * Una secci√≥n dedicada permite a los usuarios introducir su direcci√≥n de correo electr√≥nico.
    * Un bot√≥n de "Solicitar Informe" env√≠a una petici√≥n al endpoint `/generate_simple_report/` del backend (o directamente al webhook de n8n).
    * Se muestra un mensaje de estado para informar al usuario sobre el √©xito o el fallo de la solicitud.

## 9. Despliegue (Docker)

### 9.1. Descripci√≥n

Toda la infraestructura de backend (base de datos PostgreSQL, servicio Ollama, backend FastAPI y plataforma n8n) se conteneriz√≥ utilizando Docker para facilitar el despliegue, la gesti√≥n y la escalabilidad de la aplicaci√≥n.

### 9.2. Docker Compose

Se utiliza Docker Compose (`docker-compose.yml`) para definir y orquestar los diferentes servicios:

* **`postgres_db`:** Contenedor para la base de datos PostgreSQL. Se configura un volumen (`postgres_data`) para la persistencia de los datos.
* **`ollama`:** Contenedor para el servicio Ollama, encargado de ejecutar los modelos LLM localmente. Se define un volumen (`ollama_data`) para la persistencia de los modelos descargados.
* **`backend`:** Contenedor para la aplicaci√≥n FastAPI (backend). Se mapea el puerto necesario para la API y se configura para depender de los servicios `postgres_db` y `ollama`.
* **`n8n`:** Contenedor para la plataforma de automatizaci√≥n n8n. Se mapea el puerto de la interfaz de n8n y se define un volumen (`n8n_data`) para la persistencia de los workflows y la configuraci√≥n.

### 9.3. Red Interna

Se define una red interna de Docker (`housing_net`) para permitir la comunicaci√≥n entre los contenedores utilizando sus nombres de servicio como nombres de host (ej., `postgres_db`, `ollama`, `backend`, `n8n`).

### 9.4. Variables de Entorno

La configuraci√≥n sensible y espec√≠fica del entorno (URLs de la base de datos, URLs de Ollama, nombres de los modelos LLM a utilizar, claves de API para servicios externos si se usaran) se gestiona mediante variables de entorno definidas en el archivo `docker-compose.yml`. Esto permite una configuraci√≥n flexible y evita la codificaci√≥n de informaci√≥n sensible directamente en el c√≥digo.

## 10. Conclusiones

La plataforma web desarrollada proporciona un conjunto robusto de herramientas para el an√°lisis y la predicci√≥n de precios de viviendas en Barcelona. La integraci√≥n de un modelo predictivo avanzado, la capacidad de realizar consultas en lenguaje natural sobre el dataset y la automatizaci√≥n de la generaci√≥n de informes ofrecen un valor significativo para usuarios interesados en el mercado inmobiliario. La arquitectura modular y la contenerizaci√≥n con Docker facilitan el despliegue, el mantenimiento y la futura expansi√≥n de la plataforma. El uso de tecnolog√≠as modernas y eficientes en el frontend y el backend garantiza una experiencia de usuario fluida y un rendimiento adecuado.
